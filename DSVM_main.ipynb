{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.distributions import Gumbel,Bernoulli,Normal\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from torchviz import make_dot, make_dot_from_trace\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#from torchvision import datasets, transforms\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataProcessing\n",
    "from DataProcessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DSVMCode\n",
    "from DSVMCode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Model as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DataProcessing' from '/home/xiuqin/DSVM/Code/DataProcessing.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(DataProcessing)\n",
    "from DataProcessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DSVMCode' from '/home/xiuqin/DSVM/Code/DSVMCode.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(DSVMCode)\n",
    "from DSVMCode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix from 1-d time series\n",
    "def create_dataset_unsupervised(dataset, look_back=1,predict_len = 1):\n",
    "    dataY = []\n",
    "    for i in range(0,len(dataset)-look_back+1,predict_len):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataY.append(a)\n",
    "    return np.array(dataY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Big Companies: 39\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "(4527,) 3711\n",
      "3D size(Y): (2716, 10, 1) (986, 10, 1) (807, 10, 1)\n",
      "3D size(Y): (10, 105924, 1) (10, 38454, 1) (10, 31473, 1) (10, 144378, 1)\n",
      "Numpy into Tensor, done!\n"
     ]
    }
   ],
   "source": [
    "# Several companies\n",
    "sp500_df = pd.read_csv(\"../DataDsf/sp500_return_01_18.csv\")\n",
    "sp500_df = sp500_df[~sp500_df['PERMNO'].isin(list(np.unique(sp500_df.loc[sp500_df['RETX'].isnull(),'PERMNO'].values)))]\n",
    "sp500_df.rename(columns={'DATE': 'Date'}, inplace=True)\n",
    "\n",
    "bigcompany_list = pd.read_csv(\"bigcompany_list.csv\",header=None)\n",
    "print(\"Number of Big Companies:\",len(bigcompany_list))\n",
    "\n",
    "## Specify the setting of the model\n",
    "timestep = 10 # The past information used for prediction\n",
    "predict_len = 1\n",
    "predict_dim = 1 # The dimension of observations at each time step\n",
    "look_back = predict_len * timestep\n",
    "\n",
    "def ConstructDataset(sp500_df,PERMNO):\n",
    "    \n",
    "    ## The Raw data\n",
    "    RawData = sp500_df.loc[sp500_df['PERMNO'] == PERMNO]\n",
    "    RawData.reset_index(inplace=True)\n",
    "    RawData.head()\n",
    "    data = RawData['RETX'].values\n",
    "    \n",
    "\n",
    "    ## Split into train and text data, train data 0.6 vs validation 0.2 vs test data 0.2\n",
    "    length = len(data)/predict_len\n",
    "    train_len = int(length * 0.6)*predict_len\n",
    "    valid_len = int(length * 0.2)*predict_len+100-timestep\n",
    "    print(data.shape,train_len+valid_len) \n",
    "    train_valid_data = data[:(train_len+valid_len)]\n",
    "    test_data  = data[(train_len+valid_len):]\n",
    "\n",
    "    train_valid_data = train_valid_data.reshape(-1, 1)\n",
    "    test_data = test_data.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    #print(\"train size (days):\",train_data.shape[0],\"valid size(days):\",valid_data.shape[0],\"test size(days):\",test_data.shape[0])\n",
    "\n",
    "    # ## Normalize the dataset\n",
    "    # moments = normalize_moments(train_data)\n",
    "    # train_data = normalize_fit(train_data,moments)\n",
    "    # valid_data = normalize_fit(valid_data,moments)\n",
    "    # test_data = normalize_fit(test_data,moments)\n",
    "    # print('std:',train_data.std())\n",
    "\n",
    "\n",
    "    ## Create training,validation and test dataset\n",
    "    train_valid_Y = create_dataset_unsupervised(train_valid_data,look_back,predict_len)\n",
    "    testY = create_dataset_unsupervised(test_data,look_back,predict_len)\n",
    "    #print(\"2D size(Y):\", train_valid_Y.shape,testY.shape)\n",
    "\n",
    "    #Reshape to be 3D arrray\n",
    "    train_valid_Y = np.reshape(train_valid_Y, (train_valid_Y.shape[0],timestep,predict_len))\n",
    "    testY = np.reshape(testY, (testY.shape[0], timestep,predict_len))\n",
    "    \n",
    "\n",
    "    shuffle_index = torch.randperm(train_valid_Y.shape[0])\n",
    "    trainY = train_valid_Y[shuffle_index[:train_len]]\n",
    "    validY  = train_valid_Y[shuffle_index[train_len:]]\n",
    "    print(\"3D size(Y):\",trainY.shape,validY.shape,testY.shape)\n",
    "    \n",
    "    return trainY, validY, testY,train_valid_data,train_valid_Y\n",
    "\n",
    "# np.savetxt(\"bigcompany_list.csv\",bigcompany.index.values)\n",
    "\n",
    "trainY_list = []\n",
    "validY_list = []\n",
    "testY_list = []\n",
    "train_valid_Y_list = []\n",
    "train_valid_Y_list2 = []\n",
    "# for PERMNO  in list(np.unique(sp500_df['PERMNO'].values)):\n",
    "for PERMNO  in  list(bigcompany_list.values[:,0]):\n",
    "    trainY, validY, testY,train_valid_data,train_valid_Y = ConstructDataset(sp500_df,PERMNO)\n",
    "    trainY_list.append(trainY)\n",
    "    validY_list.append(validY)\n",
    "    testY_list.append(testY)\n",
    "    train_valid_Y_list.append(train_valid_data)\n",
    "    train_valid_Y_list2.append(train_valid_Y)\n",
    "    \n",
    "trainY = np.array(trainY_list).reshape(-1,timestep,1)\n",
    "validY = np.array(validY_list).reshape(-1,timestep,1)\n",
    "testY = np.array(testY_list).reshape(-1,timestep,1)\n",
    "train_valid_Y = np.array(train_valid_Y_list2).reshape(-1,timestep,1)\n",
    "\n",
    "trainY = np.transpose(trainY, (1, 0, 2))\n",
    "validY = np.transpose(validY, (1, 0, 2))\n",
    "testY = np.transpose(testY, (1, 0, 2))\n",
    "train_valid_Y = np.transpose(train_valid_Y, (1, 0, 2))\n",
    "print(\"3D size(Y):\",trainY.shape,validY.shape,testY.shape,train_valid_Y.shape)\n",
    "\n",
    "print(\"Numpy into Tensor, done!\")\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "validY = torch.from_numpy(validY).float()\n",
    "testY = torch.from_numpy(testY).float()\n",
    "train_valid_Y = torch.from_numpy(train_valid_Y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(\"Device:\",device)\n",
    "# trainX = trainX.to(device)\n",
    "# validX = validX.to(device)\n",
    "# testX = testX.to(device)\n",
    "trainY = trainY.to(device)\n",
    "validY = validY.to(device)\n",
    "testY = testY.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DSVMCode' from '/home/xiuqin/DSVM/Code/DSVMCode.py'>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(DSVMCode)\n",
    "from DSVMCode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Setting\n",
    "\n",
    "## hyperparameters\n",
    "y_dim = 1\n",
    "h_dim = 10\n",
    "z_dim = 1\n",
    "n_layers =  1\n",
    "n_epochs = 300\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 512\n",
    "\n",
    "#print_every = 100\n",
    "save_every = 1\n",
    "save_best = True\n",
    "\n",
    "directory = os.path.join(\"200906MultiCheckpoint39_10\",'{}-{}-{}'.format(n_layers,z_dim,h_dim))\n",
    "directoryBest = os.path.join(directory,'best')\n",
    "\n",
    "##manual seed\n",
    "#seed = 128\n",
    "#torch.manual_seed(seed)\n",
    "#plt.ion()\n",
    "\n",
    "##init model + optimizer\n",
    "model = DSVM(y_dim, h_dim, z_dim,n_layers,device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Epoch:1\t KLD_Gaussian Loss: 0.119065, NLL Loss: -22.308518, Loss: -22.1895\n",
      "valid Epoch:1\t KLD_Gaussian Loss: 0.118822, NLL Loss: -22.287442, Loss: -22.1686\n",
      "test Epoch:1\t KLD_Gaussian Loss: 0.118874, NLL Loss: -23.963322, Loss: -23.8444\n",
      "train Epoch:2\t KLD_Gaussian Loss: 0.019251, NLL Loss: -22.901219, Loss: -22.8820\n",
      "valid Epoch:2\t KLD_Gaussian Loss: 0.019264, NLL Loss: -22.855075, Loss: -22.8358\n",
      "test Epoch:2\t KLD_Gaussian Loss: 0.019203, NLL Loss: -24.396961, Loss: -24.3778\n",
      "train Epoch:3\t KLD_Gaussian Loss: 0.009006, NLL Loss: -23.297657, Loss: -23.2887\n",
      "valid Epoch:3\t KLD_Gaussian Loss: 0.008981, NLL Loss: -23.257771, Loss: -23.2488\n",
      "test Epoch:3\t KLD_Gaussian Loss: 0.008959, NLL Loss: -24.830347, Loss: -24.8214\n",
      "train Epoch:4\t KLD_Gaussian Loss: 0.005869, NLL Loss: -23.531364, Loss: -23.5255\n",
      "valid Epoch:4\t KLD_Gaussian Loss: 0.005869, NLL Loss: -23.489580, Loss: -23.4837\n",
      "test Epoch:4\t KLD_Gaussian Loss: 0.005874, NLL Loss: -25.078107, Loss: -25.0722\n",
      "train Epoch:5\t KLD_Gaussian Loss: 0.004224, NLL Loss: -23.678144, Loss: -23.6739\n",
      "valid Epoch:5\t KLD_Gaussian Loss: 0.004219, NLL Loss: -23.638563, Loss: -23.6343\n",
      "test Epoch:5\t KLD_Gaussian Loss: 0.004196, NLL Loss: -25.244195, Loss: -25.2400\n",
      "train Epoch:6\t KLD_Gaussian Loss: 0.003766, NLL Loss: -23.751418, Loss: -23.7477\n",
      "valid Epoch:6\t KLD_Gaussian Loss: 0.003776, NLL Loss: -23.719665, Loss: -23.7159\n",
      "test Epoch:6\t KLD_Gaussian Loss: 0.003750, NLL Loss: -25.328730, Loss: -25.3250\n",
      "train Epoch:7\t KLD_Gaussian Loss: 0.002376, NLL Loss: -23.808389, Loss: -23.8060\n",
      "valid Epoch:7\t KLD_Gaussian Loss: 0.002373, NLL Loss: -23.779098, Loss: -23.7767\n",
      "test Epoch:7\t KLD_Gaussian Loss: 0.002351, NLL Loss: -25.393127, Loss: -25.3908\n",
      "train Epoch:8\t KLD_Gaussian Loss: 0.001538, NLL Loss: -23.839116, Loss: -23.8376\n",
      "valid Epoch:8\t KLD_Gaussian Loss: 0.001539, NLL Loss: -23.805212, Loss: -23.8037\n",
      "test Epoch:8\t KLD_Gaussian Loss: 0.001496, NLL Loss: -25.425762, Loss: -25.4243\n",
      "train Epoch:9\t KLD_Gaussian Loss: 0.000989, NLL Loss: -23.860546, Loss: -23.8596\n",
      "valid Epoch:9\t KLD_Gaussian Loss: 0.000990, NLL Loss: -23.828662, Loss: -23.8277\n",
      "test Epoch:9\t KLD_Gaussian Loss: 0.000945, NLL Loss: -25.456415, Loss: -25.4555\n",
      "train Epoch:10\t KLD_Gaussian Loss: 0.001212, NLL Loss: -23.876997, Loss: -23.8758\n",
      "valid Epoch:10\t KLD_Gaussian Loss: 0.001215, NLL Loss: -23.847179, Loss: -23.8460\n",
      "test Epoch:10\t KLD_Gaussian Loss: 0.001173, NLL Loss: -25.475753, Loss: -25.4746\n",
      "train Epoch:11\t KLD_Gaussian Loss: 0.001023, NLL Loss: -23.876891, Loss: -23.8759\n",
      "valid Epoch:11\t KLD_Gaussian Loss: 0.001027, NLL Loss: -23.846407, Loss: -23.8454\n",
      "test Epoch:11\t KLD_Gaussian Loss: 0.000982, NLL Loss: -25.474309, Loss: -25.4733\n",
      "train Epoch:12\t KLD_Gaussian Loss: 0.000619, NLL Loss: -23.885104, Loss: -23.8845\n",
      "valid Epoch:12\t KLD_Gaussian Loss: 0.000621, NLL Loss: -23.856046, Loss: -23.8554\n",
      "test Epoch:12\t KLD_Gaussian Loss: 0.000574, NLL Loss: -25.484308, Loss: -25.4837\n",
      "train Epoch:13\t KLD_Gaussian Loss: 0.000574, NLL Loss: -23.890070, Loss: -23.8895\n",
      "valid Epoch:13\t KLD_Gaussian Loss: 0.000577, NLL Loss: -23.859985, Loss: -23.8594\n",
      "test Epoch:13\t KLD_Gaussian Loss: 0.000528, NLL Loss: -25.490732, Loss: -25.4902\n",
      "train Epoch:14\t KLD_Gaussian Loss: 0.000467, NLL Loss: -23.898026, Loss: -23.8976\n",
      "valid Epoch:14\t KLD_Gaussian Loss: 0.000470, NLL Loss: -23.866844, Loss: -23.8664\n",
      "test Epoch:14\t KLD_Gaussian Loss: 0.000423, NLL Loss: -25.504484, Loss: -25.5041\n",
      "train Epoch:15\t KLD_Gaussian Loss: 0.000301, NLL Loss: -23.900589, Loss: -23.9003\n",
      "valid Epoch:15\t KLD_Gaussian Loss: 0.000303, NLL Loss: -23.871688, Loss: -23.8714\n",
      "test Epoch:15\t KLD_Gaussian Loss: 0.000257, NLL Loss: -25.509951, Loss: -25.5097\n",
      "train Epoch:16\t KLD_Gaussian Loss: 0.000305, NLL Loss: -23.898887, Loss: -23.8986\n",
      "valid Epoch:16\t KLD_Gaussian Loss: 0.000307, NLL Loss: -23.869939, Loss: -23.8696\n",
      "test Epoch:16\t KLD_Gaussian Loss: 0.000258, NLL Loss: -25.506706, Loss: -25.5064\n",
      "train Epoch:17\t KLD_Gaussian Loss: 0.000303, NLL Loss: -23.903870, Loss: -23.9036\n",
      "valid Epoch:17\t KLD_Gaussian Loss: 0.000305, NLL Loss: -23.873890, Loss: -23.8736\n",
      "test Epoch:17\t KLD_Gaussian Loss: 0.000259, NLL Loss: -25.511851, Loss: -25.5116\n",
      "train Epoch:18\t KLD_Gaussian Loss: 0.000341, NLL Loss: -23.901715, Loss: -23.9014\n",
      "valid Epoch:18\t KLD_Gaussian Loss: 0.000343, NLL Loss: -23.873228, Loss: -23.8729\n",
      "test Epoch:18\t KLD_Gaussian Loss: 0.000297, NLL Loss: -25.507737, Loss: -25.5074\n",
      "train Epoch:19\t KLD_Gaussian Loss: 0.000312, NLL Loss: -23.904948, Loss: -23.9046\n",
      "valid Epoch:19\t KLD_Gaussian Loss: 0.000315, NLL Loss: -23.875104, Loss: -23.8748\n",
      "test Epoch:19\t KLD_Gaussian Loss: 0.000275, NLL Loss: -25.512703, Loss: -25.5124\n",
      "train Epoch:20\t KLD_Gaussian Loss: 0.000357, NLL Loss: -23.907082, Loss: -23.9067\n",
      "valid Epoch:20\t KLD_Gaussian Loss: 0.000359, NLL Loss: -23.878504, Loss: -23.8781\n",
      "test Epoch:20\t KLD_Gaussian Loss: 0.000328, NLL Loss: -25.515303, Loss: -25.5150\n",
      "train Epoch:21\t KLD_Gaussian Loss: 0.000314, NLL Loss: -23.905803, Loss: -23.9055\n",
      "valid Epoch:21\t KLD_Gaussian Loss: 0.000316, NLL Loss: -23.878384, Loss: -23.8781\n",
      "test Epoch:21\t KLD_Gaussian Loss: 0.000280, NLL Loss: -25.515368, Loss: -25.5151\n",
      "train Epoch:22\t KLD_Gaussian Loss: 0.000425, NLL Loss: -23.907181, Loss: -23.9068\n",
      "valid Epoch:22\t KLD_Gaussian Loss: 0.000428, NLL Loss: -23.882108, Loss: -23.8817\n",
      "test Epoch:22\t KLD_Gaussian Loss: 0.000391, NLL Loss: -25.518883, Loss: -25.5185\n",
      "train Epoch:23\t KLD_Gaussian Loss: 0.000289, NLL Loss: -23.906877, Loss: -23.9066\n",
      "valid Epoch:23\t KLD_Gaussian Loss: 0.000292, NLL Loss: -23.878522, Loss: -23.8782\n",
      "test Epoch:23\t KLD_Gaussian Loss: 0.000246, NLL Loss: -25.516836, Loss: -25.5166\n",
      "train Epoch:24\t KLD_Gaussian Loss: 0.000277, NLL Loss: -23.907717, Loss: -23.9074\n",
      "valid Epoch:24\t KLD_Gaussian Loss: 0.000282, NLL Loss: -23.881501, Loss: -23.8812\n",
      "test Epoch:24\t KLD_Gaussian Loss: 0.000231, NLL Loss: -25.519421, Loss: -25.5192\n",
      "train Epoch:25\t KLD_Gaussian Loss: 0.000270, NLL Loss: -23.908562, Loss: -23.9083\n",
      "valid Epoch:25\t KLD_Gaussian Loss: 0.000276, NLL Loss: -23.880609, Loss: -23.8803\n",
      "test Epoch:25\t KLD_Gaussian Loss: 0.000222, NLL Loss: -25.519503, Loss: -25.5193\n",
      "train Epoch:26\t KLD_Gaussian Loss: 0.000221, NLL Loss: -23.908260, Loss: -23.9080\n",
      "valid Epoch:26\t KLD_Gaussian Loss: 0.000227, NLL Loss: -23.881095, Loss: -23.8809\n",
      "test Epoch:26\t KLD_Gaussian Loss: 0.000163, NLL Loss: -25.519265, Loss: -25.5191\n",
      "train Epoch:27\t KLD_Gaussian Loss: 0.000210, NLL Loss: -23.909596, Loss: -23.9094\n",
      "valid Epoch:27\t KLD_Gaussian Loss: 0.000219, NLL Loss: -23.883614, Loss: -23.8834\n",
      "test Epoch:27\t KLD_Gaussian Loss: 0.000122, NLL Loss: -25.521068, Loss: -25.5209\n",
      "train Epoch:28\t KLD_Gaussian Loss: 0.000228, NLL Loss: -23.909758, Loss: -23.9095\n",
      "valid Epoch:28\t KLD_Gaussian Loss: 0.000238, NLL Loss: -23.883200, Loss: -23.8830\n",
      "test Epoch:28\t KLD_Gaussian Loss: 0.000137, NLL Loss: -25.521360, Loss: -25.5212\n",
      "train Epoch:29\t KLD_Gaussian Loss: 0.000243, NLL Loss: -23.910596, Loss: -23.9104\n",
      "valid Epoch:29\t KLD_Gaussian Loss: 0.000258, NLL Loss: -23.882395, Loss: -23.8821\n",
      "test Epoch:29\t KLD_Gaussian Loss: 0.000123, NLL Loss: -25.521352, Loss: -25.5212\n",
      "train Epoch:30\t KLD_Gaussian Loss: 0.000264, NLL Loss: -23.912244, Loss: -23.9120\n",
      "valid Epoch:30\t KLD_Gaussian Loss: 0.000278, NLL Loss: -23.885111, Loss: -23.8848\n",
      "test Epoch:30\t KLD_Gaussian Loss: 0.000116, NLL Loss: -25.524036, Loss: -25.5239\n",
      "train Epoch:31\t KLD_Gaussian Loss: 0.000335, NLL Loss: -23.913051, Loss: -23.9127\n",
      "valid Epoch:31\t KLD_Gaussian Loss: 0.000355, NLL Loss: -23.885712, Loss: -23.8854\n",
      "test Epoch:31\t KLD_Gaussian Loss: 0.000134, NLL Loss: -25.524342, Loss: -25.5242\n",
      "train Epoch:32\t KLD_Gaussian Loss: 0.000571, NLL Loss: -23.913587, Loss: -23.9130\n",
      "valid Epoch:32\t KLD_Gaussian Loss: 0.000607, NLL Loss: -23.886901, Loss: -23.8863\n",
      "test Epoch:32\t KLD_Gaussian Loss: 0.000192, NLL Loss: -25.524372, Loss: -25.5242\n",
      "train Epoch:33\t KLD_Gaussian Loss: 0.000857, NLL Loss: -23.915510, Loss: -23.9147\n",
      "valid Epoch:33\t KLD_Gaussian Loss: 0.000890, NLL Loss: -23.888682, Loss: -23.8878\n",
      "test Epoch:33\t KLD_Gaussian Loss: 0.000378, NLL Loss: -25.526954, Loss: -25.5266\n",
      "train Epoch:34\t KLD_Gaussian Loss: 0.001131, NLL Loss: -23.917689, Loss: -23.9166\n",
      "valid Epoch:34\t KLD_Gaussian Loss: 0.001194, NLL Loss: -23.891123, Loss: -23.8899\n",
      "test Epoch:34\t KLD_Gaussian Loss: 0.000404, NLL Loss: -25.528536, Loss: -25.5281\n",
      "train Epoch:35\t KLD_Gaussian Loss: 0.001374, NLL Loss: -23.917903, Loss: -23.9165\n",
      "valid Epoch:35\t KLD_Gaussian Loss: 0.001442, NLL Loss: -23.890301, Loss: -23.8889\n",
      "test Epoch:35\t KLD_Gaussian Loss: 0.000489, NLL Loss: -25.527530, Loss: -25.5270\n",
      "train Epoch:36\t KLD_Gaussian Loss: 0.001860, NLL Loss: -23.921800, Loss: -23.9199\n",
      "valid Epoch:36\t KLD_Gaussian Loss: 0.001933, NLL Loss: -23.894444, Loss: -23.8925\n",
      "test Epoch:36\t KLD_Gaussian Loss: 0.000735, NLL Loss: -25.529148, Loss: -25.5284\n",
      "train Epoch:37\t KLD_Gaussian Loss: 0.002415, NLL Loss: -23.924814, Loss: -23.9224\n",
      "valid Epoch:37\t KLD_Gaussian Loss: 0.002503, NLL Loss: -23.899131, Loss: -23.8966\n",
      "test Epoch:37\t KLD_Gaussian Loss: 0.000797, NLL Loss: -25.530792, Loss: -25.5300\n",
      "train Epoch:38\t KLD_Gaussian Loss: 0.003136, NLL Loss: -23.934673, Loss: -23.9315\n",
      "valid Epoch:38\t KLD_Gaussian Loss: 0.003270, NLL Loss: -23.909741, Loss: -23.9065\n",
      "test Epoch:38\t KLD_Gaussian Loss: 0.000698, NLL Loss: -25.539864, Loss: -25.5392\n",
      "train Epoch:39\t KLD_Gaussian Loss: 0.004591, NLL Loss: -23.947484, Loss: -23.9429\n",
      "valid Epoch:39\t KLD_Gaussian Loss: 0.004735, NLL Loss: -23.919917, Loss: -23.9152\n",
      "test Epoch:39\t KLD_Gaussian Loss: 0.000974, NLL Loss: -25.552845, Loss: -25.5519\n",
      "train Epoch:40\t KLD_Gaussian Loss: 0.007984, NLL Loss: -23.969542, Loss: -23.9616\n",
      "valid Epoch:40\t KLD_Gaussian Loss: 0.008257, NLL Loss: -23.946764, Loss: -23.9385\n",
      "test Epoch:40\t KLD_Gaussian Loss: 0.001749, NLL Loss: -25.572846, Loss: -25.5711\n",
      "train Epoch:41\t KLD_Gaussian Loss: 0.013515, NLL Loss: -24.013550, Loss: -24.0000\n",
      "valid Epoch:41\t KLD_Gaussian Loss: 0.013809, NLL Loss: -23.993598, Loss: -23.9798\n",
      "test Epoch:41\t KLD_Gaussian Loss: 0.002933, NLL Loss: -25.599246, Loss: -25.5963\n",
      "train Epoch:42\t KLD_Gaussian Loss: 0.028578, NLL Loss: -24.112229, Loss: -24.0837\n",
      "valid Epoch:42\t KLD_Gaussian Loss: 0.029095, NLL Loss: -24.090145, Loss: -24.0611\n",
      "test Epoch:42\t KLD_Gaussian Loss: 0.006564, NLL Loss: -25.665896, Loss: -25.6593\n",
      "train Epoch:43\t KLD_Gaussian Loss: 0.072645, NLL Loss: -24.483233, Loss: -24.4106\n",
      "valid Epoch:43\t KLD_Gaussian Loss: 0.073400, NLL Loss: -24.463633, Loss: -24.3902\n",
      "test Epoch:43\t KLD_Gaussian Loss: 0.015376, NLL Loss: -26.029887, Loss: -26.0145\n",
      "train Epoch:44\t KLD_Gaussian Loss: 0.147204, NLL Loss: -25.013354, Loss: -24.8662\n",
      "valid Epoch:44\t KLD_Gaussian Loss: 0.149043, NLL Loss: -25.024702, Loss: -24.8757\n",
      "test Epoch:44\t KLD_Gaussian Loss: 0.037677, NLL Loss: -26.621557, Loss: -26.5839\n",
      "train Epoch:45\t KLD_Gaussian Loss: 0.257741, NLL Loss: -25.593100, Loss: -25.3354\n",
      "valid Epoch:45\t KLD_Gaussian Loss: 0.261105, NLL Loss: -25.574414, Loss: -25.3133\n",
      "test Epoch:45\t KLD_Gaussian Loss: 0.079585, NLL Loss: -27.178812, Loss: -27.0992\n",
      "train Epoch:46\t KLD_Gaussian Loss: 0.379497, NLL Loss: -26.225846, Loss: -25.8463\n",
      "valid Epoch:46\t KLD_Gaussian Loss: 0.383380, NLL Loss: -26.220736, Loss: -25.8374\n",
      "test Epoch:46\t KLD_Gaussian Loss: 0.123212, NLL Loss: -27.866499, Loss: -27.7433\n",
      "train Epoch:47\t KLD_Gaussian Loss: 0.531054, NLL Loss: -26.751036, Loss: -26.2200\n",
      "valid Epoch:47\t KLD_Gaussian Loss: 0.535153, NLL Loss: -26.742803, Loss: -26.2077\n",
      "test Epoch:47\t KLD_Gaussian Loss: 0.276455, NLL Loss: -28.566772, Loss: -28.2903\n",
      "train Epoch:48\t KLD_Gaussian Loss: 0.696457, NLL Loss: -27.063918, Loss: -26.3675\n",
      "valid Epoch:48\t KLD_Gaussian Loss: 0.700493, NLL Loss: -27.059981, Loss: -26.3595\n",
      "test Epoch:48\t KLD_Gaussian Loss: 0.455700, NLL Loss: -29.003682, Loss: -28.5480\n",
      "train Epoch:49\t KLD_Gaussian Loss: 0.731607, NLL Loss: -27.217644, Loss: -26.4860\n",
      "valid Epoch:49\t KLD_Gaussian Loss: 0.734809, NLL Loss: -27.234550, Loss: -26.4997\n",
      "test Epoch:49\t KLD_Gaussian Loss: 0.467902, NLL Loss: -29.145446, Loss: -28.6775\n",
      "train Epoch:50\t KLD_Gaussian Loss: 0.727091, NLL Loss: -27.307782, Loss: -26.5807\n",
      "valid Epoch:50\t KLD_Gaussian Loss: 0.731288, NLL Loss: -27.285321, Loss: -26.5540\n",
      "test Epoch:50\t KLD_Gaussian Loss: 0.430093, NLL Loss: -29.205454, Loss: -28.7754\n",
      "train Epoch:51\t KLD_Gaussian Loss: 0.767689, NLL Loss: -27.378998, Loss: -26.6113\n",
      "valid Epoch:51\t KLD_Gaussian Loss: 0.771692, NLL Loss: -27.372965, Loss: -26.6013\n",
      "test Epoch:51\t KLD_Gaussian Loss: 0.468773, NLL Loss: -29.286055, Loss: -28.8173\n",
      "train Epoch:52\t KLD_Gaussian Loss: 0.761233, NLL Loss: -27.404455, Loss: -26.6432\n",
      "valid Epoch:52\t KLD_Gaussian Loss: 0.766516, NLL Loss: -27.398359, Loss: -26.6318\n",
      "test Epoch:52\t KLD_Gaussian Loss: 0.456937, NLL Loss: -29.310540, Loss: -28.8536\n",
      "train Epoch:53\t KLD_Gaussian Loss: 0.771190, NLL Loss: -27.410221, Loss: -26.6390\n",
      "valid Epoch:53\t KLD_Gaussian Loss: 0.776274, NLL Loss: -27.414557, Loss: -26.6383\n",
      "test Epoch:53\t KLD_Gaussian Loss: 0.449865, NLL Loss: -29.268953, Loss: -28.8191\n",
      "train Epoch:54\t KLD_Gaussian Loss: 0.788374, NLL Loss: -27.451019, Loss: -26.6626\n",
      "valid Epoch:54\t KLD_Gaussian Loss: 0.792383, NLL Loss: -27.456272, Loss: -26.6639\n",
      "test Epoch:54\t KLD_Gaussian Loss: 0.482096, NLL Loss: -29.357145, Loss: -28.8750\n",
      "train Epoch:55\t KLD_Gaussian Loss: 0.806930, NLL Loss: -27.484616, Loss: -26.6777\n",
      "valid Epoch:55\t KLD_Gaussian Loss: 0.812899, NLL Loss: -27.450363, Loss: -26.6375\n",
      "test Epoch:55\t KLD_Gaussian Loss: 0.509994, NLL Loss: -29.414852, Loss: -28.9049\n",
      "train Epoch:56\t KLD_Gaussian Loss: 0.814220, NLL Loss: -27.511015, Loss: -26.6968\n",
      "valid Epoch:56\t KLD_Gaussian Loss: 0.819540, NLL Loss: -27.495985, Loss: -26.6764\n",
      "test Epoch:56\t KLD_Gaussian Loss: 0.523692, NLL Loss: -29.444639, Loss: -28.9209\n",
      "train Epoch:66\t KLD_Gaussian Loss: 0.945133, NLL Loss: -27.792974, Loss: -26.8478\n",
      "valid Epoch:66\t KLD_Gaussian Loss: 0.949033, NLL Loss: -27.783439, Loss: -26.8344\n",
      "test Epoch:66\t KLD_Gaussian Loss: 0.719537, NLL Loss: -29.799086, Loss: -29.0795\n",
      "train Epoch:67\t KLD_Gaussian Loss: 0.954438, NLL Loss: -27.825139, Loss: -26.8707\n",
      "valid Epoch:67\t KLD_Gaussian Loss: 0.958328, NLL Loss: -27.815673, Loss: -26.8573\n",
      "test Epoch:67\t KLD_Gaussian Loss: 0.730623, NLL Loss: -29.846593, Loss: -29.1160\n",
      "train Epoch:68\t KLD_Gaussian Loss: 0.958395, NLL Loss: -27.844134, Loss: -26.8857\n",
      "valid Epoch:68\t KLD_Gaussian Loss: 0.961456, NLL Loss: -27.831890, Loss: -26.8704\n",
      "test Epoch:68\t KLD_Gaussian Loss: 0.734562, NLL Loss: -29.861697, Loss: -29.1271\n",
      "train Epoch:69\t KLD_Gaussian Loss: 0.954151, NLL Loss: -27.865760, Loss: -26.9116\n",
      "valid Epoch:69\t KLD_Gaussian Loss: 0.958835, NLL Loss: -27.849606, Loss: -26.8908\n",
      "test Epoch:69\t KLD_Gaussian Loss: 0.723208, NLL Loss: -29.861600, Loss: -29.1384\n",
      "train Epoch:70\t KLD_Gaussian Loss: 0.944969, NLL Loss: -27.866159, Loss: -26.9212\n",
      "valid Epoch:70\t KLD_Gaussian Loss: 0.950105, NLL Loss: -27.861497, Loss: -26.9114\n",
      "test Epoch:70\t KLD_Gaussian Loss: 0.715994, NLL Loss: -29.890591, Loss: -29.1746\n",
      "train Epoch:71\t KLD_Gaussian Loss: 0.978981, NLL Loss: -27.917743, Loss: -26.9388\n",
      "valid Epoch:71\t KLD_Gaussian Loss: 0.979870, NLL Loss: -27.902533, Loss: -26.9227\n",
      "test Epoch:71\t KLD_Gaussian Loss: 0.756875, NLL Loss: -29.935236, Loss: -29.1784\n",
      "train Epoch:72\t KLD_Gaussian Loss: 1.028196, NLL Loss: -27.957415, Loss: -26.9292\n",
      "valid Epoch:72\t KLD_Gaussian Loss: 1.030928, NLL Loss: -27.949485, Loss: -26.9186\n",
      "test Epoch:72\t KLD_Gaussian Loss: 0.819271, NLL Loss: -29.999311, Loss: -29.1800\n",
      "train Epoch:73\t KLD_Gaussian Loss: 0.986180, NLL Loss: -27.948378, Loss: -26.9622\n",
      "valid Epoch:73\t KLD_Gaussian Loss: 0.989104, NLL Loss: -27.934168, Loss: -26.9451\n",
      "test Epoch:73\t KLD_Gaussian Loss: 0.750464, NLL Loss: -29.962259, Loss: -29.2118\n",
      "train Epoch:74\t KLD_Gaussian Loss: 0.989434, NLL Loss: -27.960309, Loss: -26.9709\n",
      "valid Epoch:74\t KLD_Gaussian Loss: 0.992222, NLL Loss: -27.954420, Loss: -26.9622\n",
      "test Epoch:74\t KLD_Gaussian Loss: 0.760210, NLL Loss: -29.972897, Loss: -29.2127\n",
      "train Epoch:75\t KLD_Gaussian Loss: 0.999853, NLL Loss: -27.963526, Loss: -26.9637\n",
      "valid Epoch:75\t KLD_Gaussian Loss: 1.000897, NLL Loss: -27.943601, Loss: -26.9427\n",
      "test Epoch:75\t KLD_Gaussian Loss: 0.783247, NLL Loss: -29.994577, Loss: -29.2113\n",
      "train Epoch:76\t KLD_Gaussian Loss: 1.052401, NLL Loss: -28.031527, Loss: -26.9791\n",
      "valid Epoch:76\t KLD_Gaussian Loss: 1.055297, NLL Loss: -28.014348, Loss: -26.9591\n",
      "test Epoch:76\t KLD_Gaussian Loss: 0.836450, NLL Loss: -30.063938, Loss: -29.2275\n",
      "train Epoch:77\t KLD_Gaussian Loss: 1.033832, NLL Loss: -28.023734, Loss: -26.9899\n",
      "valid Epoch:77\t KLD_Gaussian Loss: 1.036431, NLL Loss: -28.006761, Loss: -26.9703\n",
      "test Epoch:77\t KLD_Gaussian Loss: 0.798126, NLL Loss: -30.036944, Loss: -29.2388\n",
      "train Epoch:78\t KLD_Gaussian Loss: 0.996143, NLL Loss: -27.997054, Loss: -27.0009\n",
      "valid Epoch:78\t KLD_Gaussian Loss: 0.999003, NLL Loss: -27.990456, Loss: -26.9915\n",
      "test Epoch:78\t KLD_Gaussian Loss: 0.749197, NLL Loss: -29.999263, Loss: -29.2501\n",
      "train Epoch:79\t KLD_Gaussian Loss: 1.010206, NLL Loss: -28.005778, Loss: -26.9956\n",
      "valid Epoch:79\t KLD_Gaussian Loss: 1.011822, NLL Loss: -28.009992, Loss: -26.9982\n",
      "test Epoch:79\t KLD_Gaussian Loss: 0.758548, NLL Loss: -30.024170, Loss: -29.2656\n",
      "train Epoch:80\t KLD_Gaussian Loss: 1.043638, NLL Loss: -28.072252, Loss: -27.0286\n",
      "valid Epoch:80\t KLD_Gaussian Loss: 1.044659, NLL Loss: -28.063592, Loss: -27.0189\n",
      "test Epoch:80\t KLD_Gaussian Loss: 0.820799, NLL Loss: -30.118224, Loss: -29.2974\n",
      "train Epoch:81\t KLD_Gaussian Loss: 1.018569, NLL Loss: -28.044537, Loss: -27.0260\n",
      "valid Epoch:81\t KLD_Gaussian Loss: 1.020244, NLL Loss: -28.044212, Loss: -27.0240\n",
      "test Epoch:81\t KLD_Gaussian Loss: 0.770271, NLL Loss: -30.050412, Loss: -29.2801\n",
      "train Epoch:82\t KLD_Gaussian Loss: 1.014222, NLL Loss: -28.040605, Loss: -27.0264\n",
      "valid Epoch:82\t KLD_Gaussian Loss: 1.016191, NLL Loss: -28.026951, Loss: -27.0108\n",
      "test Epoch:82\t KLD_Gaussian Loss: 0.767197, NLL Loss: -30.068700, Loss: -29.3015\n",
      "train Epoch:83\t KLD_Gaussian Loss: 1.059952, NLL Loss: -28.113973, Loss: -27.0540\n",
      "valid Epoch:83\t KLD_Gaussian Loss: 1.060067, NLL Loss: -28.110990, Loss: -27.0509\n",
      "test Epoch:83\t KLD_Gaussian Loss: 0.837832, NLL Loss: -30.176510, Loss: -29.3387\n",
      "train Epoch:84\t KLD_Gaussian Loss: 1.054679, NLL Loss: -28.098823, Loss: -27.0441\n",
      "valid Epoch:84\t KLD_Gaussian Loss: 1.053635, NLL Loss: -28.103328, Loss: -27.0497\n",
      "test Epoch:84\t KLD_Gaussian Loss: 0.821680, NLL Loss: -30.144386, Loss: -29.3227\n",
      "train Epoch:85\t KLD_Gaussian Loss: 1.048095, NLL Loss: -28.122135, Loss: -27.0740\n",
      "valid Epoch:85\t KLD_Gaussian Loss: 1.050462, NLL Loss: -28.104150, Loss: -27.0537\n",
      "test Epoch:85\t KLD_Gaussian Loss: 0.817882, NLL Loss: -30.154911, Loss: -29.3370\n",
      "train Epoch:86\t KLD_Gaussian Loss: 1.075989, NLL Loss: -28.153587, Loss: -27.0776\n",
      "valid Epoch:86\t KLD_Gaussian Loss: 1.076419, NLL Loss: -28.149227, Loss: -27.0728\n",
      "test Epoch:86\t KLD_Gaussian Loss: 0.846535, NLL Loss: -30.206224, Loss: -29.3597\n",
      "train Epoch:87\t KLD_Gaussian Loss: 1.008186, NLL Loss: -28.060888, Loss: -27.0527\n",
      "valid Epoch:87\t KLD_Gaussian Loss: 1.011756, NLL Loss: -28.043529, Loss: -27.0318\n",
      "test Epoch:87\t KLD_Gaussian Loss: 0.736794, NLL Loss: -30.069748, Loss: -29.3330\n",
      "train Epoch:88\t KLD_Gaussian Loss: 1.087942, NLL Loss: -28.186601, Loss: -27.0987\n",
      "valid Epoch:88\t KLD_Gaussian Loss: 1.089170, NLL Loss: -28.171643, Loss: -27.0825\n",
      "test Epoch:88\t KLD_Gaussian Loss: 0.871737, NLL Loss: -30.243961, Loss: -29.3722\n",
      "train Epoch:89\t KLD_Gaussian Loss: 1.065361, NLL Loss: -28.170042, Loss: -27.1047\n",
      "valid Epoch:89\t KLD_Gaussian Loss: 1.069915, NLL Loss: -28.155488, Loss: -27.0856\n",
      "test Epoch:89\t KLD_Gaussian Loss: 0.822485, NLL Loss: -30.206645, Loss: -29.3842\n",
      "train Epoch:90\t KLD_Gaussian Loss: 1.098992, NLL Loss: -28.212162, Loss: -27.1132\n",
      "valid Epoch:90\t KLD_Gaussian Loss: 1.102557, NLL Loss: -28.189824, Loss: -27.0873\n",
      "test Epoch:90\t KLD_Gaussian Loss: 0.875714, NLL Loss: -30.280524, Loss: -29.4048\n",
      "train Epoch:91\t KLD_Gaussian Loss: 1.085928, NLL Loss: -28.198468, Loss: -27.1125\n",
      "valid Epoch:91\t KLD_Gaussian Loss: 1.087864, NLL Loss: -28.195328, Loss: -27.1075\n",
      "test Epoch:91\t KLD_Gaussian Loss: 0.858690, NLL Loss: -30.270649, Loss: -29.4120\n",
      "train Epoch:92\t KLD_Gaussian Loss: 1.058974, NLL Loss: -28.165694, Loss: -27.1067\n",
      "valid Epoch:92\t KLD_Gaussian Loss: 1.059768, NLL Loss: -28.169209, Loss: -27.1094\n",
      "test Epoch:92\t KLD_Gaussian Loss: 0.807005, NLL Loss: -30.208959, Loss: -29.4020\n",
      "train Epoch:93\t KLD_Gaussian Loss: 1.040742, NLL Loss: -28.154675, Loss: -27.1139\n",
      "valid Epoch:93\t KLD_Gaussian Loss: 1.040930, NLL Loss: -28.145612, Loss: -27.1047\n",
      "test Epoch:93\t KLD_Gaussian Loss: 0.790409, NLL Loss: -30.184827, Loss: -29.3944\n",
      "train Epoch:94\t KLD_Gaussian Loss: 1.099172, NLL Loss: -28.222188, Loss: -27.1230\n",
      "valid Epoch:94\t KLD_Gaussian Loss: 1.099154, NLL Loss: -28.208109, Loss: -27.1090\n",
      "test Epoch:94\t KLD_Gaussian Loss: 0.862121, NLL Loss: -30.290292, Loss: -29.4282\n",
      "train Epoch:95\t KLD_Gaussian Loss: 1.083127, NLL Loss: -28.206181, Loss: -27.1231\n",
      "valid Epoch:95\t KLD_Gaussian Loss: 1.082901, NLL Loss: -28.205083, Loss: -27.1222\n",
      "test Epoch:95\t KLD_Gaussian Loss: 0.847108, NLL Loss: -30.253038, Loss: -29.4059\n",
      "train Epoch:96\t KLD_Gaussian Loss: 1.058611, NLL Loss: -28.179423, Loss: -27.1208\n",
      "valid Epoch:96\t KLD_Gaussian Loss: 1.061361, NLL Loss: -28.162386, Loss: -27.1010\n",
      "test Epoch:96\t KLD_Gaussian Loss: 0.794568, NLL Loss: -30.239896, Loss: -29.4453\n",
      "train Epoch:97\t KLD_Gaussian Loss: 1.073369, NLL Loss: -28.209377, Loss: -27.1360\n",
      "valid Epoch:97\t KLD_Gaussian Loss: 1.074996, NLL Loss: -28.207072, Loss: -27.1321\n",
      "test Epoch:97\t KLD_Gaussian Loss: 0.837196, NLL Loss: -30.268035, Loss: -29.4308\n",
      "train Epoch:98\t KLD_Gaussian Loss: 1.089512, NLL Loss: -28.235355, Loss: -27.1458\n",
      "valid Epoch:98\t KLD_Gaussian Loss: 1.087664, NLL Loss: -28.229462, Loss: -27.1418\n",
      "test Epoch:98\t KLD_Gaussian Loss: 0.850081, NLL Loss: -30.288841, Loss: -29.4388\n",
      "train Epoch:99\t KLD_Gaussian Loss: 1.071451, NLL Loss: -28.216065, Loss: -27.1446\n",
      "valid Epoch:99\t KLD_Gaussian Loss: 1.072940, NLL Loss: -28.205655, Loss: -27.1327\n",
      "test Epoch:99\t KLD_Gaussian Loss: 0.821270, NLL Loss: -30.264056, Loss: -29.4428\n",
      "train Epoch:100\t KLD_Gaussian Loss: 1.091029, NLL Loss: -28.247371, Loss: -27.1563\n",
      "valid Epoch:100\t KLD_Gaussian Loss: 1.088063, NLL Loss: -28.235132, Loss: -27.1471\n",
      "test Epoch:100\t KLD_Gaussian Loss: 0.865016, NLL Loss: -30.339354, Loss: -29.4743\n",
      "train Epoch:101\t KLD_Gaussian Loss: 1.075999, NLL Loss: -28.241395, Loss: -27.1654\n",
      "valid Epoch:101\t KLD_Gaussian Loss: 1.076913, NLL Loss: -28.230103, Loss: -27.1532\n",
      "test Epoch:101\t KLD_Gaussian Loss: 0.842640, NLL Loss: -30.317399, Loss: -29.4748\n",
      "train Epoch:102\t KLD_Gaussian Loss: 1.076017, NLL Loss: -28.218739, Loss: -27.1427\n",
      "valid Epoch:102\t KLD_Gaussian Loss: 1.077267, NLL Loss: -28.208912, Loss: -27.1316\n",
      "test Epoch:102\t KLD_Gaussian Loss: 0.833619, NLL Loss: -30.278088, Loss: -29.4445\n",
      "train Epoch:103\t KLD_Gaussian Loss: 1.080632, NLL Loss: -28.241723, Loss: -27.1611\n",
      "valid Epoch:103\t KLD_Gaussian Loss: 1.081534, NLL Loss: -28.236799, Loss: -27.1553\n",
      "test Epoch:103\t KLD_Gaussian Loss: 0.853910, NLL Loss: -30.317419, Loss: -29.4635\n",
      "train Epoch:104\t KLD_Gaussian Loss: 1.062888, NLL Loss: -28.215150, Loss: -27.1523\n",
      "valid Epoch:104\t KLD_Gaussian Loss: 1.065684, NLL Loss: -28.208431, Loss: -27.1427\n",
      "test Epoch:104\t KLD_Gaussian Loss: 0.811358, NLL Loss: -30.271133, Loss: -29.4598\n",
      "train Epoch:105\t KLD_Gaussian Loss: 1.076717, NLL Loss: -28.248709, Loss: -27.1720\n",
      "valid Epoch:105\t KLD_Gaussian Loss: 1.077113, NLL Loss: -28.240710, Loss: -27.1636\n",
      "test Epoch:105\t KLD_Gaussian Loss: 0.853853, NLL Loss: -30.333862, Loss: -29.4800\n",
      "train Epoch:106\t KLD_Gaussian Loss: 1.082585, NLL Loss: -28.270534, Loss: -27.1879\n",
      "valid Epoch:106\t KLD_Gaussian Loss: 1.084924, NLL Loss: -28.255169, Loss: -27.1702\n",
      "test Epoch:106\t KLD_Gaussian Loss: 0.872275, NLL Loss: -30.372941, Loss: -29.5007\n",
      "train Epoch:107\t KLD_Gaussian Loss: 1.069738, NLL Loss: -28.242950, Loss: -27.1732\n",
      "valid Epoch:107\t KLD_Gaussian Loss: 1.070647, NLL Loss: -28.227369, Loss: -27.1567\n",
      "test Epoch:107\t KLD_Gaussian Loss: 0.823847, NLL Loss: -30.314432, Loss: -29.4906\n",
      "train Epoch:108\t KLD_Gaussian Loss: 1.082780, NLL Loss: -28.263776, Loss: -27.1810\n",
      "valid Epoch:108\t KLD_Gaussian Loss: 1.081521, NLL Loss: -28.255588, Loss: -27.1741\n",
      "test Epoch:108\t KLD_Gaussian Loss: 0.865630, NLL Loss: -30.348231, Loss: -29.4826\n",
      "train Epoch:109\t KLD_Gaussian Loss: 1.098918, NLL Loss: -28.297468, Loss: -27.1986\n",
      "valid Epoch:109\t KLD_Gaussian Loss: 1.100944, NLL Loss: -28.280829, Loss: -27.1799\n",
      "test Epoch:109\t KLD_Gaussian Loss: 0.886717, NLL Loss: -30.379317, Loss: -29.4926\n",
      "train Epoch:110\t KLD_Gaussian Loss: 1.048437, NLL Loss: -28.221295, Loss: -27.1729\n",
      "valid Epoch:110\t KLD_Gaussian Loss: 1.048016, NLL Loss: -28.216414, Loss: -27.1684\n",
      "test Epoch:110\t KLD_Gaussian Loss: 0.811884, NLL Loss: -30.296306, Loss: -29.4844\n",
      "train Epoch:111\t KLD_Gaussian Loss: 1.053323, NLL Loss: -28.223200, Loss: -27.1699\n",
      "valid Epoch:111\t KLD_Gaussian Loss: 1.052877, NLL Loss: -28.204475, Loss: -27.1516\n",
      "test Epoch:111\t KLD_Gaussian Loss: 0.813157, NLL Loss: -30.287290, Loss: -29.4741\n",
      "train Epoch:112\t KLD_Gaussian Loss: 1.088191, NLL Loss: -28.285112, Loss: -27.1969\n",
      "valid Epoch:112\t KLD_Gaussian Loss: 1.084890, NLL Loss: -28.282308, Loss: -27.1974\n",
      "test Epoch:112\t KLD_Gaussian Loss: 0.877847, NLL Loss: -30.365484, Loss: -29.4876\n",
      "train Epoch:113\t KLD_Gaussian Loss: 1.077920, NLL Loss: -28.269158, Loss: -27.1912\n",
      "valid Epoch:113\t KLD_Gaussian Loss: 1.079697, NLL Loss: -28.260418, Loss: -27.1807\n",
      "test Epoch:113\t KLD_Gaussian Loss: 0.862872, NLL Loss: -30.342248, Loss: -29.4794\n",
      "train Epoch:114\t KLD_Gaussian Loss: 1.049514, NLL Loss: -28.238657, Loss: -27.1891\n",
      "valid Epoch:114\t KLD_Gaussian Loss: 1.051492, NLL Loss: -28.231237, Loss: -27.1797\n",
      "test Epoch:114\t KLD_Gaussian Loss: 0.821881, NLL Loss: -30.317165, Loss: -29.4953\n",
      "train Epoch:115\t KLD_Gaussian Loss: 1.077737, NLL Loss: -28.269816, Loss: -27.1921\n",
      "valid Epoch:115\t KLD_Gaussian Loss: 1.075653, NLL Loss: -28.265480, Loss: -27.1898\n",
      "test Epoch:115\t KLD_Gaussian Loss: 0.850117, NLL Loss: -30.347492, Loss: -29.4974\n",
      "train Epoch:116\t KLD_Gaussian Loss: 1.053954, NLL Loss: -28.236823, Loss: -27.1829\n",
      "valid Epoch:116\t KLD_Gaussian Loss: 1.056334, NLL Loss: -28.210183, Loss: -27.1538\n",
      "test Epoch:116\t KLD_Gaussian Loss: 0.818943, NLL Loss: -30.287969, Loss: -29.4690\n",
      "train Epoch:117\t KLD_Gaussian Loss: 1.035026, NLL Loss: -28.221569, Loss: -27.1865\n",
      "valid Epoch:117\t KLD_Gaussian Loss: 1.035376, NLL Loss: -28.209165, Loss: -27.1738\n",
      "test Epoch:117\t KLD_Gaussian Loss: 0.803522, NLL Loss: -30.291694, Loss: -29.4882\n",
      "train Epoch:118\t KLD_Gaussian Loss: 1.057400, NLL Loss: -28.239299, Loss: -27.1819\n",
      "valid Epoch:118\t KLD_Gaussian Loss: 1.057459, NLL Loss: -28.230259, Loss: -27.1728\n",
      "test Epoch:118\t KLD_Gaussian Loss: 0.825617, NLL Loss: -30.301514, Loss: -29.4759\n",
      "train Epoch:119\t KLD_Gaussian Loss: 1.113636, NLL Loss: -28.379942, Loss: -27.2663\n",
      "valid Epoch:119\t KLD_Gaussian Loss: 1.116434, NLL Loss: -28.359644, Loss: -27.2432\n",
      "test Epoch:119\t KLD_Gaussian Loss: 0.842042, NLL Loss: -30.521399, Loss: -29.6794\n",
      "train Epoch:120\t KLD_Gaussian Loss: 1.122342, NLL Loss: -28.394080, Loss: -27.2717\n",
      "valid Epoch:120\t KLD_Gaussian Loss: 1.121893, NLL Loss: -28.372829, Loss: -27.2509\n",
      "test Epoch:120\t KLD_Gaussian Loss: 0.849917, NLL Loss: -30.542791, Loss: -29.6929\n",
      "train Epoch:121\t KLD_Gaussian Loss: 1.126693, NLL Loss: -28.401590, Loss: -27.2749\n",
      "valid Epoch:121\t KLD_Gaussian Loss: 1.127670, NLL Loss: -28.379856, Loss: -27.2522\n",
      "test Epoch:121\t KLD_Gaussian Loss: 0.850980, NLL Loss: -30.552164, Loss: -29.7012\n",
      "train Epoch:122\t KLD_Gaussian Loss: 1.121011, NLL Loss: -28.404682, Loss: -27.2837\n",
      "valid Epoch:122\t KLD_Gaussian Loss: 1.124239, NLL Loss: -28.383162, Loss: -27.2589\n",
      "test Epoch:122\t KLD_Gaussian Loss: 0.837690, NLL Loss: -30.549759, Loss: -29.7121\n",
      "train Epoch:123\t KLD_Gaussian Loss: 1.133156, NLL Loss: -28.405999, Loss: -27.2728\n",
      "valid Epoch:123\t KLD_Gaussian Loss: 1.134057, NLL Loss: -28.394114, Loss: -27.2601\n",
      "test Epoch:123\t KLD_Gaussian Loss: 0.858571, NLL Loss: -30.557811, Loss: -29.6992\n",
      "train Epoch:124\t KLD_Gaussian Loss: 1.127229, NLL Loss: -28.400792, Loss: -27.2736\n",
      "valid Epoch:124\t KLD_Gaussian Loss: 1.125935, NLL Loss: -28.393265, Loss: -27.2673\n",
      "test Epoch:124\t KLD_Gaussian Loss: 0.850163, NLL Loss: -30.547275, Loss: -29.6971\n",
      "train Epoch:125\t KLD_Gaussian Loss: 1.135245, NLL Loss: -28.410620, Loss: -27.2754\n",
      "valid Epoch:125\t KLD_Gaussian Loss: 1.134971, NLL Loss: -28.396835, Loss: -27.2619\n",
      "test Epoch:125\t KLD_Gaussian Loss: 0.860160, NLL Loss: -30.553149, Loss: -29.6930\n",
      "train Epoch:126\t KLD_Gaussian Loss: 1.130616, NLL Loss: -28.405595, Loss: -27.2750\n",
      "valid Epoch:126\t KLD_Gaussian Loss: 1.129217, NLL Loss: -28.394442, Loss: -27.2652\n",
      "test Epoch:126\t KLD_Gaussian Loss: 0.846861, NLL Loss: -30.564486, Loss: -29.7176\n",
      "train Epoch:127\t KLD_Gaussian Loss: 1.122683, NLL Loss: -28.398439, Loss: -27.2758\n",
      "valid Epoch:127\t KLD_Gaussian Loss: 1.123456, NLL Loss: -28.386192, Loss: -27.2627\n",
      "test Epoch:127\t KLD_Gaussian Loss: 0.838034, NLL Loss: -30.553413, Loss: -29.7154\n",
      "train Epoch:128\t KLD_Gaussian Loss: 1.128791, NLL Loss: -28.407070, Loss: -27.2783\n",
      "valid Epoch:128\t KLD_Gaussian Loss: 1.131273, NLL Loss: -28.391195, Loss: -27.2599\n",
      "test Epoch:128\t KLD_Gaussian Loss: 0.852587, NLL Loss: -30.562325, Loss: -29.7097\n",
      "train Epoch:129\t KLD_Gaussian Loss: 1.121180, NLL Loss: -28.398904, Loss: -27.2777\n",
      "valid Epoch:129\t KLD_Gaussian Loss: 1.119446, NLL Loss: -28.384810, Loss: -27.2654\n",
      "test Epoch:129\t KLD_Gaussian Loss: 0.839529, NLL Loss: -30.559591, Loss: -29.7201\n",
      "train Epoch:130\t KLD_Gaussian Loss: 1.126785, NLL Loss: -28.397877, Loss: -27.2711\n",
      "valid Epoch:130\t KLD_Gaussian Loss: 1.127912, NLL Loss: -28.400179, Loss: -27.2723\n",
      "test Epoch:130\t KLD_Gaussian Loss: 0.846176, NLL Loss: -30.552752, Loss: -29.7066\n",
      "train Epoch:131\t KLD_Gaussian Loss: 1.123956, NLL Loss: -28.394254, Loss: -27.2703\n",
      "valid Epoch:131\t KLD_Gaussian Loss: 1.126247, NLL Loss: -28.393174, Loss: -27.2669\n",
      "test Epoch:131\t KLD_Gaussian Loss: 0.842196, NLL Loss: -30.549203, Loss: -29.7070\n",
      "train Epoch:132\t KLD_Gaussian Loss: 1.128525, NLL Loss: -28.407934, Loss: -27.2794\n",
      "valid Epoch:132\t KLD_Gaussian Loss: 1.133361, NLL Loss: -28.381953, Loss: -27.2486\n",
      "test Epoch:132\t KLD_Gaussian Loss: 0.850629, NLL Loss: -30.563596, Loss: -29.7130\n",
      "train Epoch:133\t KLD_Gaussian Loss: 1.125645, NLL Loss: -28.401741, Loss: -27.2761\n",
      "valid Epoch:133\t KLD_Gaussian Loss: 1.132245, NLL Loss: -28.389131, Loss: -27.2569\n",
      "test Epoch:133\t KLD_Gaussian Loss: 0.839427, NLL Loss: -30.537983, Loss: -29.6986\n",
      "train Epoch:134\t KLD_Gaussian Loss: 1.112468, NLL Loss: -28.395732, Loss: -27.2833\n",
      "valid Epoch:134\t KLD_Gaussian Loss: 1.113695, NLL Loss: -28.381384, Loss: -27.2677\n",
      "test Epoch:134\t KLD_Gaussian Loss: 0.824375, NLL Loss: -30.542838, Loss: -29.7185\n",
      "train Epoch:135\t KLD_Gaussian Loss: 1.119509, NLL Loss: -28.399119, Loss: -27.2796\n",
      "valid Epoch:135\t KLD_Gaussian Loss: 1.120091, NLL Loss: -28.374418, Loss: -27.2543\n",
      "test Epoch:135\t KLD_Gaussian Loss: 0.835612, NLL Loss: -30.563020, Loss: -29.7274\n",
      "train Epoch:136\t KLD_Gaussian Loss: 1.115300, NLL Loss: -28.394903, Loss: -27.2796\n",
      "valid Epoch:136\t KLD_Gaussian Loss: 1.113365, NLL Loss: -28.381651, Loss: -27.2683\n",
      "test Epoch:136\t KLD_Gaussian Loss: 0.831472, NLL Loss: -30.545126, Loss: -29.7137\n",
      "train Epoch:137\t KLD_Gaussian Loss: 1.126303, NLL Loss: -28.407054, Loss: -27.2808\n",
      "valid Epoch:137\t KLD_Gaussian Loss: 1.125826, NLL Loss: -28.395583, Loss: -27.2698\n",
      "test Epoch:137\t KLD_Gaussian Loss: 0.848401, NLL Loss: -30.572026, Loss: -29.7236\n",
      "train Epoch:138\t KLD_Gaussian Loss: 1.127327, NLL Loss: -28.409022, Loss: -27.2817\n",
      "valid Epoch:138\t KLD_Gaussian Loss: 1.127837, NLL Loss: -28.398577, Loss: -27.2707\n",
      "test Epoch:138\t KLD_Gaussian Loss: 0.841676, NLL Loss: -30.586330, Loss: -29.7447\n",
      "train Epoch:139\t KLD_Gaussian Loss: 1.129178, NLL Loss: -28.415378, Loss: -27.2862\n",
      "valid Epoch:139\t KLD_Gaussian Loss: 1.135112, NLL Loss: -28.392641, Loss: -27.2575\n",
      "test Epoch:139\t KLD_Gaussian Loss: 0.842485, NLL Loss: -30.577705, Loss: -29.7352\n",
      "train Epoch:140\t KLD_Gaussian Loss: 1.129166, NLL Loss: -28.418293, Loss: -27.2891\n",
      "valid Epoch:140\t KLD_Gaussian Loss: 1.127834, NLL Loss: -28.410227, Loss: -27.2824\n",
      "test Epoch:140\t KLD_Gaussian Loss: 0.834043, NLL Loss: -30.586751, Loss: -29.7527\n",
      "train Epoch:141\t KLD_Gaussian Loss: 1.133471, NLL Loss: -28.417554, Loss: -27.2841\n",
      "valid Epoch:141\t KLD_Gaussian Loss: 1.132499, NLL Loss: -28.405387, Loss: -27.2729\n",
      "test Epoch:141\t KLD_Gaussian Loss: 0.844807, NLL Loss: -30.594263, Loss: -29.7495\n",
      "train Epoch:142\t KLD_Gaussian Loss: 1.134528, NLL Loss: -28.421751, Loss: -27.2872\n",
      "valid Epoch:142\t KLD_Gaussian Loss: 1.133485, NLL Loss: -28.405696, Loss: -27.2722\n",
      "test Epoch:142\t KLD_Gaussian Loss: 0.837942, NLL Loss: -30.598336, Loss: -29.7604\n",
      "train Epoch:143\t KLD_Gaussian Loss: 1.131895, NLL Loss: -28.416136, Loss: -27.2842\n",
      "valid Epoch:143\t KLD_Gaussian Loss: 1.133476, NLL Loss: -28.402760, Loss: -27.2693\n",
      "test Epoch:143\t KLD_Gaussian Loss: 0.832478, NLL Loss: -30.586606, Loss: -29.7541\n",
      "train Epoch:144\t KLD_Gaussian Loss: 1.134639, NLL Loss: -28.416950, Loss: -27.2823\n",
      "valid Epoch:144\t KLD_Gaussian Loss: 1.133813, NLL Loss: -28.411180, Loss: -27.2774\n",
      "test Epoch:144\t KLD_Gaussian Loss: 0.834267, NLL Loss: -30.605054, Loss: -29.7708\n",
      "train Epoch:145\t KLD_Gaussian Loss: 1.130608, NLL Loss: -28.415508, Loss: -27.2849\n",
      "valid Epoch:145\t KLD_Gaussian Loss: 1.131527, NLL Loss: -28.400869, Loss: -27.2693\n",
      "test Epoch:145\t KLD_Gaussian Loss: 0.827349, NLL Loss: -30.603178, Loss: -29.7758\n",
      "train Epoch:146\t KLD_Gaussian Loss: 1.135073, NLL Loss: -28.421361, Loss: -27.2863\n",
      "valid Epoch:146\t KLD_Gaussian Loss: 1.133632, NLL Loss: -28.413351, Loss: -27.2797\n",
      "test Epoch:146\t KLD_Gaussian Loss: 0.835035, NLL Loss: -30.593068, Loss: -29.7580\n",
      "train Epoch:147\t KLD_Gaussian Loss: 1.140520, NLL Loss: -28.424510, Loss: -27.2840\n",
      "valid Epoch:147\t KLD_Gaussian Loss: 1.138631, NLL Loss: -28.423213, Loss: -27.2846\n",
      "test Epoch:147\t KLD_Gaussian Loss: 0.842966, NLL Loss: -30.604850, Loss: -29.7619\n",
      "train Epoch:148\t KLD_Gaussian Loss: 1.136638, NLL Loss: -28.414590, Loss: -27.2780\n",
      "valid Epoch:148\t KLD_Gaussian Loss: 1.135165, NLL Loss: -28.406144, Loss: -27.2710\n",
      "test Epoch:148\t KLD_Gaussian Loss: 0.833860, NLL Loss: -30.602117, Loss: -29.7683\n",
      "train Epoch:149\t KLD_Gaussian Loss: 1.136139, NLL Loss: -28.420481, Loss: -27.2843\n",
      "valid Epoch:149\t KLD_Gaussian Loss: 1.136481, NLL Loss: -28.407113, Loss: -27.2706\n",
      "test Epoch:149\t KLD_Gaussian Loss: 0.836680, NLL Loss: -30.599152, Loss: -29.7625\n",
      "train Epoch:150\t KLD_Gaussian Loss: 1.134698, NLL Loss: -28.424189, Loss: -27.2895\n",
      "valid Epoch:150\t KLD_Gaussian Loss: 1.137452, NLL Loss: -28.406723, Loss: -27.2693\n",
      "test Epoch:150\t KLD_Gaussian Loss: 0.831455, NLL Loss: -30.597359, Loss: -29.7659\n",
      "train Epoch:151\t KLD_Gaussian Loss: 1.137358, NLL Loss: -28.431293, Loss: -27.2939\n",
      "valid Epoch:151\t KLD_Gaussian Loss: 1.137448, NLL Loss: -28.411687, Loss: -27.2742\n",
      "test Epoch:151\t KLD_Gaussian Loss: 0.840045, NLL Loss: -30.604566, Loss: -29.7645\n",
      "train Epoch:152\t KLD_Gaussian Loss: 1.134835, NLL Loss: -28.421916, Loss: -27.2871\n",
      "valid Epoch:152\t KLD_Gaussian Loss: 1.132562, NLL Loss: -28.415805, Loss: -27.2832\n",
      "test Epoch:152\t KLD_Gaussian Loss: 0.835012, NLL Loss: -30.584308, Loss: -29.7493\n",
      "train Epoch:153\t KLD_Gaussian Loss: 1.137624, NLL Loss: -28.432503, Loss: -27.2949\n",
      "valid Epoch:153\t KLD_Gaussian Loss: 1.140246, NLL Loss: -28.414564, Loss: -27.2743\n",
      "test Epoch:153\t KLD_Gaussian Loss: 0.841371, NLL Loss: -30.596664, Loss: -29.7553\n",
      "train Epoch:154\t KLD_Gaussian Loss: 1.138147, NLL Loss: -28.421198, Loss: -27.2831\n",
      "valid Epoch:154\t KLD_Gaussian Loss: 1.138103, NLL Loss: -28.412207, Loss: -27.2741\n",
      "test Epoch:154\t KLD_Gaussian Loss: 0.834894, NLL Loss: -30.604445, Loss: -29.7696\n",
      "train Epoch:155\t KLD_Gaussian Loss: 1.130994, NLL Loss: -28.417398, Loss: -27.2864\n",
      "valid Epoch:155\t KLD_Gaussian Loss: 1.129647, NLL Loss: -28.408069, Loss: -27.2784\n",
      "test Epoch:155\t KLD_Gaussian Loss: 0.823487, NLL Loss: -30.592559, Loss: -29.7691\n",
      "train Epoch:156\t KLD_Gaussian Loss: 1.132190, NLL Loss: -28.427198, Loss: -27.2950\n",
      "valid Epoch:156\t KLD_Gaussian Loss: 1.132391, NLL Loss: -28.410542, Loss: -27.2782\n",
      "test Epoch:156\t KLD_Gaussian Loss: 0.832251, NLL Loss: -30.591501, Loss: -29.7593\n",
      "train Epoch:157\t KLD_Gaussian Loss: 1.135939, NLL Loss: -28.418163, Loss: -27.2822\n",
      "valid Epoch:157\t KLD_Gaussian Loss: 1.136742, NLL Loss: -28.421673, Loss: -27.2849\n",
      "test Epoch:157\t KLD_Gaussian Loss: 0.834097, NLL Loss: -30.615752, Loss: -29.7817\n",
      "train Epoch:158\t KLD_Gaussian Loss: 1.135416, NLL Loss: -28.420327, Loss: -27.2849\n",
      "valid Epoch:158\t KLD_Gaussian Loss: 1.135123, NLL Loss: -28.417281, Loss: -27.2822\n",
      "test Epoch:158\t KLD_Gaussian Loss: 0.830811, NLL Loss: -30.600415, Loss: -29.7696\n",
      "train Epoch:159\t KLD_Gaussian Loss: 1.133422, NLL Loss: -28.413325, Loss: -27.2799\n",
      "valid Epoch:159\t KLD_Gaussian Loss: 1.133702, NLL Loss: -28.405735, Loss: -27.2720\n",
      "test Epoch:159\t KLD_Gaussian Loss: 0.831203, NLL Loss: -30.596889, Loss: -29.7657\n",
      "train Epoch:160\t KLD_Gaussian Loss: 1.137972, NLL Loss: -28.427382, Loss: -27.2894\n",
      "valid Epoch:160\t KLD_Gaussian Loss: 1.137686, NLL Loss: -28.397634, Loss: -27.2599\n",
      "test Epoch:160\t KLD_Gaussian Loss: 0.840014, NLL Loss: -30.612368, Loss: -29.7724\n",
      "train Epoch:161\t KLD_Gaussian Loss: 1.136944, NLL Loss: -28.423053, Loss: -27.2861\n",
      "valid Epoch:161\t KLD_Gaussian Loss: 1.137239, NLL Loss: -28.400482, Loss: -27.2632\n",
      "test Epoch:161\t KLD_Gaussian Loss: 0.832755, NLL Loss: -30.605448, Loss: -29.7727\n",
      "train Epoch:162\t KLD_Gaussian Loss: 1.135890, NLL Loss: -28.422978, Loss: -27.2871\n",
      "valid Epoch:162\t KLD_Gaussian Loss: 1.138174, NLL Loss: -28.408303, Loss: -27.2701\n",
      "test Epoch:162\t KLD_Gaussian Loss: 0.832630, NLL Loss: -30.600332, Loss: -29.7677\n",
      "train Epoch:163\t KLD_Gaussian Loss: 1.135594, NLL Loss: -28.422551, Loss: -27.2870\n",
      "valid Epoch:163\t KLD_Gaussian Loss: 1.136124, NLL Loss: -28.410299, Loss: -27.2742\n",
      "test Epoch:163\t KLD_Gaussian Loss: 0.837104, NLL Loss: -30.595747, Loss: -29.7586\n",
      "train Epoch:164\t KLD_Gaussian Loss: 1.135326, NLL Loss: -28.425671, Loss: -27.2903\n",
      "valid Epoch:164\t KLD_Gaussian Loss: 1.139436, NLL Loss: -28.409840, Loss: -27.2704\n",
      "test Epoch:164\t KLD_Gaussian Loss: 0.832343, NLL Loss: -30.596873, Loss: -29.7645\n",
      "train Epoch:165\t KLD_Gaussian Loss: 1.137683, NLL Loss: -28.421918, Loss: -27.2842\n",
      "valid Epoch:165\t KLD_Gaussian Loss: 1.135514, NLL Loss: -28.420252, Loss: -27.2847\n",
      "test Epoch:165\t KLD_Gaussian Loss: 0.833601, NLL Loss: -30.605849, Loss: -29.7722\n",
      "train Epoch:166\t KLD_Gaussian Loss: 1.134969, NLL Loss: -28.425211, Loss: -27.2902\n",
      "valid Epoch:166\t KLD_Gaussian Loss: 1.136329, NLL Loss: -28.422095, Loss: -27.2858\n",
      "test Epoch:166\t KLD_Gaussian Loss: 0.835401, NLL Loss: -30.602830, Loss: -29.7674\n",
      "train Epoch:167\t KLD_Gaussian Loss: 1.136504, NLL Loss: -28.417113, Loss: -27.2806\n",
      "valid Epoch:167\t KLD_Gaussian Loss: 1.133633, NLL Loss: -28.423903, Loss: -27.2903\n",
      "test Epoch:167\t KLD_Gaussian Loss: 0.835249, NLL Loss: -30.600906, Loss: -29.7657\n",
      "train Epoch:168\t KLD_Gaussian Loss: 1.135066, NLL Loss: -28.430004, Loss: -27.2949\n",
      "valid Epoch:168\t KLD_Gaussian Loss: 1.136974, NLL Loss: -28.408114, Loss: -27.2711\n",
      "test Epoch:168\t KLD_Gaussian Loss: 0.832511, NLL Loss: -30.601452, Loss: -29.7689\n",
      "train Epoch:169\t KLD_Gaussian Loss: 1.135367, NLL Loss: -28.419506, Loss: -27.2841\n",
      "valid Epoch:169\t KLD_Gaussian Loss: 1.134341, NLL Loss: -28.407558, Loss: -27.2732\n",
      "test Epoch:169\t KLD_Gaussian Loss: 0.833608, NLL Loss: -30.603617, Loss: -29.7700\n",
      "train Epoch:170\t KLD_Gaussian Loss: 1.136540, NLL Loss: -28.416681, Loss: -27.2801\n",
      "valid Epoch:170\t KLD_Gaussian Loss: 1.133110, NLL Loss: -28.415639, Loss: -27.2825\n",
      "test Epoch:170\t KLD_Gaussian Loss: 0.832398, NLL Loss: -30.611657, Loss: -29.7793\n",
      "train Epoch:171\t KLD_Gaussian Loss: 1.136517, NLL Loss: -28.421092, Loss: -27.2846\n",
      "valid Epoch:171\t KLD_Gaussian Loss: 1.136641, NLL Loss: -28.414840, Loss: -27.2782\n",
      "test Epoch:171\t KLD_Gaussian Loss: 0.831809, NLL Loss: -30.603356, Loss: -29.7715\n",
      "train Epoch:172\t KLD_Gaussian Loss: 1.137876, NLL Loss: -28.421250, Loss: -27.2834\n",
      "valid Epoch:172\t KLD_Gaussian Loss: 1.134548, NLL Loss: -28.416553, Loss: -27.2820\n",
      "test Epoch:172\t KLD_Gaussian Loss: 0.833636, NLL Loss: -30.593747, Loss: -29.7601\n",
      "train Epoch:173\t KLD_Gaussian Loss: 1.135092, NLL Loss: -28.430280, Loss: -27.2952\n",
      "valid Epoch:173\t KLD_Gaussian Loss: 1.136018, NLL Loss: -28.410588, Loss: -27.2746\n",
      "test Epoch:173\t KLD_Gaussian Loss: 0.834112, NLL Loss: -30.600076, Loss: -29.7660\n",
      "train Epoch:174\t KLD_Gaussian Loss: 1.135591, NLL Loss: -28.421540, Loss: -27.2859\n",
      "valid Epoch:174\t KLD_Gaussian Loss: 1.135203, NLL Loss: -28.408264, Loss: -27.2731\n",
      "test Epoch:174\t KLD_Gaussian Loss: 0.833227, NLL Loss: -30.609932, Loss: -29.7767\n",
      "train Epoch:175\t KLD_Gaussian Loss: 1.135272, NLL Loss: -28.414979, Loss: -27.2797\n",
      "valid Epoch:175\t KLD_Gaussian Loss: 1.134366, NLL Loss: -28.399113, Loss: -27.2647\n",
      "test Epoch:175\t KLD_Gaussian Loss: 0.831437, NLL Loss: -30.601114, Loss: -29.7697\n",
      "train Epoch:176\t KLD_Gaussian Loss: 1.135332, NLL Loss: -28.421467, Loss: -27.2861\n",
      "valid Epoch:176\t KLD_Gaussian Loss: 1.137702, NLL Loss: -28.411391, Loss: -27.2737\n",
      "test Epoch:176\t KLD_Gaussian Loss: 0.833125, NLL Loss: -30.589110, Loss: -29.7560\n",
      "train Epoch:177\t KLD_Gaussian Loss: 1.134035, NLL Loss: -28.425815, Loss: -27.2918\n",
      "valid Epoch:177\t KLD_Gaussian Loss: 1.136400, NLL Loss: -28.414472, Loss: -27.2781\n",
      "test Epoch:177\t KLD_Gaussian Loss: 0.830323, NLL Loss: -30.597123, Loss: -29.7668\n",
      "train Epoch:178\t KLD_Gaussian Loss: 1.134109, NLL Loss: -28.422201, Loss: -27.2881\n",
      "valid Epoch:178\t KLD_Gaussian Loss: 1.134122, NLL Loss: -28.420460, Loss: -27.2863\n",
      "test Epoch:178\t KLD_Gaussian Loss: 0.832665, NLL Loss: -30.597034, Loss: -29.7644\n",
      "train Epoch:179\t KLD_Gaussian Loss: 1.135891, NLL Loss: -28.416475, Loss: -27.2806\n",
      "valid Epoch:179\t KLD_Gaussian Loss: 1.135878, NLL Loss: -28.409606, Loss: -27.2737\n",
      "test Epoch:179\t KLD_Gaussian Loss: 0.830846, NLL Loss: -30.598354, Loss: -29.7675\n",
      "train Epoch:180\t KLD_Gaussian Loss: 1.135907, NLL Loss: -28.421668, Loss: -27.2858\n",
      "valid Epoch:180\t KLD_Gaussian Loss: 1.135698, NLL Loss: -28.408553, Loss: -27.2729\n",
      "test Epoch:180\t KLD_Gaussian Loss: 0.832463, NLL Loss: -30.598426, Loss: -29.7660\n",
      "train Epoch:181\t KLD_Gaussian Loss: 1.134277, NLL Loss: -28.419334, Loss: -27.2851\n",
      "valid Epoch:181\t KLD_Gaussian Loss: 1.135343, NLL Loss: -28.423578, Loss: -27.2882\n",
      "test Epoch:181\t KLD_Gaussian Loss: 0.833596, NLL Loss: -30.588937, Loss: -29.7553\n",
      "train Epoch:182\t KLD_Gaussian Loss: 1.134395, NLL Loss: -28.420634, Loss: -27.2862\n",
      "valid Epoch:182\t KLD_Gaussian Loss: 1.135458, NLL Loss: -28.410838, Loss: -27.2754\n",
      "test Epoch:182\t KLD_Gaussian Loss: 0.834041, NLL Loss: -30.596698, Loss: -29.7627\n",
      "train Epoch:183\t KLD_Gaussian Loss: 1.136122, NLL Loss: -28.417165, Loss: -27.2810\n",
      "valid Epoch:183\t KLD_Gaussian Loss: 1.135336, NLL Loss: -28.409697, Loss: -27.2744\n",
      "test Epoch:183\t KLD_Gaussian Loss: 0.835162, NLL Loss: -30.599204, Loss: -29.7640\n",
      "train Epoch:184\t KLD_Gaussian Loss: 1.135610, NLL Loss: -28.418295, Loss: -27.2827\n",
      "valid Epoch:184\t KLD_Gaussian Loss: 1.136574, NLL Loss: -28.400066, Loss: -27.2635\n",
      "test Epoch:184\t KLD_Gaussian Loss: 0.833781, NLL Loss: -30.594156, Loss: -29.7604\n",
      "train Epoch:185\t KLD_Gaussian Loss: 1.135895, NLL Loss: -28.426400, Loss: -27.2905\n",
      "valid Epoch:185\t KLD_Gaussian Loss: 1.137937, NLL Loss: -28.396561, Loss: -27.2586\n",
      "test Epoch:185\t KLD_Gaussian Loss: 0.831625, NLL Loss: -30.599830, Loss: -29.7682\n",
      "train Epoch:186\t KLD_Gaussian Loss: 1.134707, NLL Loss: -28.427009, Loss: -27.2923\n",
      "valid Epoch:186\t KLD_Gaussian Loss: 1.134469, NLL Loss: -28.412269, Loss: -27.2778\n",
      "test Epoch:186\t KLD_Gaussian Loss: 0.833857, NLL Loss: -30.596019, Loss: -29.7622\n",
      "train Epoch:187\t KLD_Gaussian Loss: 1.134032, NLL Loss: -28.426646, Loss: -27.2926\n",
      "valid Epoch:187\t KLD_Gaussian Loss: 1.134186, NLL Loss: -28.413283, Loss: -27.2791\n",
      "test Epoch:187\t KLD_Gaussian Loss: 0.832004, NLL Loss: -30.600731, Loss: -29.7687\n",
      "train Epoch:188\t KLD_Gaussian Loss: 1.134640, NLL Loss: -28.417235, Loss: -27.2826\n",
      "valid Epoch:188\t KLD_Gaussian Loss: 1.135680, NLL Loss: -28.412772, Loss: -27.2771\n",
      "test Epoch:188\t KLD_Gaussian Loss: 0.833295, NLL Loss: -30.600687, Loss: -29.7674\n",
      "train Epoch:189\t KLD_Gaussian Loss: 1.134909, NLL Loss: -28.424387, Loss: -27.2895\n",
      "valid Epoch:189\t KLD_Gaussian Loss: 1.133422, NLL Loss: -28.406437, Loss: -27.2730\n",
      "test Epoch:189\t KLD_Gaussian Loss: 0.831217, NLL Loss: -30.602089, Loss: -29.7709\n",
      "train Epoch:190\t KLD_Gaussian Loss: 1.133983, NLL Loss: -28.423346, Loss: -27.2894\n",
      "valid Epoch:190\t KLD_Gaussian Loss: 1.135608, NLL Loss: -28.416153, Loss: -27.2805\n",
      "test Epoch:190\t KLD_Gaussian Loss: 0.829604, NLL Loss: -30.610478, Loss: -29.7809\n",
      "train Epoch:191\t KLD_Gaussian Loss: 1.133678, NLL Loss: -28.423596, Loss: -27.2899\n",
      "valid Epoch:191\t KLD_Gaussian Loss: 1.135123, NLL Loss: -28.406794, Loss: -27.2717\n",
      "test Epoch:191\t KLD_Gaussian Loss: 0.831489, NLL Loss: -30.609372, Loss: -29.7779\n",
      "train Epoch:192\t KLD_Gaussian Loss: 1.134047, NLL Loss: -28.430712, Loss: -27.2967\n",
      "valid Epoch:192\t KLD_Gaussian Loss: 1.134385, NLL Loss: -28.412018, Loss: -27.2776\n",
      "test Epoch:192\t KLD_Gaussian Loss: 0.834272, NLL Loss: -30.598420, Loss: -29.7641\n",
      "train Epoch:193\t KLD_Gaussian Loss: 1.134741, NLL Loss: -28.428118, Loss: -27.2934\n",
      "valid Epoch:193\t KLD_Gaussian Loss: 1.134590, NLL Loss: -28.412659, Loss: -27.2781\n",
      "test Epoch:193\t KLD_Gaussian Loss: 0.831379, NLL Loss: -30.596889, Loss: -29.7655\n",
      "train Epoch:194\t KLD_Gaussian Loss: 1.135395, NLL Loss: -28.420509, Loss: -27.2851\n",
      "valid Epoch:194\t KLD_Gaussian Loss: 1.134462, NLL Loss: -28.412974, Loss: -27.2785\n",
      "test Epoch:194\t KLD_Gaussian Loss: 0.832779, NLL Loss: -30.602354, Loss: -29.7696\n",
      "train Epoch:195\t KLD_Gaussian Loss: 1.134558, NLL Loss: -28.420627, Loss: -27.2861\n",
      "valid Epoch:195\t KLD_Gaussian Loss: 1.138114, NLL Loss: -28.397520, Loss: -27.2594\n",
      "test Epoch:195\t KLD_Gaussian Loss: 0.831306, NLL Loss: -30.597320, Loss: -29.7660\n",
      "train Epoch:196\t KLD_Gaussian Loss: 1.134443, NLL Loss: -28.424958, Loss: -27.2905\n",
      "valid Epoch:196\t KLD_Gaussian Loss: 1.134694, NLL Loss: -28.405166, Loss: -27.2705\n",
      "test Epoch:196\t KLD_Gaussian Loss: 0.829157, NLL Loss: -30.594661, Loss: -29.7655\n",
      "train Epoch:197\t KLD_Gaussian Loss: 1.136769, NLL Loss: -28.416058, Loss: -27.2793\n",
      "valid Epoch:197\t KLD_Gaussian Loss: 1.136045, NLL Loss: -28.418815, Loss: -27.2828\n",
      "test Epoch:197\t KLD_Gaussian Loss: 0.829238, NLL Loss: -30.602433, Loss: -29.7732\n",
      "train Epoch:198\t KLD_Gaussian Loss: 1.134299, NLL Loss: -28.422300, Loss: -27.2880\n",
      "valid Epoch:198\t KLD_Gaussian Loss: 1.134296, NLL Loss: -28.411859, Loss: -27.2776\n",
      "test Epoch:198\t KLD_Gaussian Loss: 0.831691, NLL Loss: -30.601216, Loss: -29.7695\n",
      "train Epoch:199\t KLD_Gaussian Loss: 1.133589, NLL Loss: -28.424023, Loss: -27.2904\n",
      "valid Epoch:199\t KLD_Gaussian Loss: 1.135751, NLL Loss: -28.409483, Loss: -27.2737\n",
      "test Epoch:199\t KLD_Gaussian Loss: 0.833470, NLL Loss: -30.598406, Loss: -29.7649\n",
      "train Epoch:200\t KLD_Gaussian Loss: 1.134767, NLL Loss: -28.417986, Loss: -27.2832\n",
      "valid Epoch:200\t KLD_Gaussian Loss: 1.135307, NLL Loss: -28.411963, Loss: -27.2767\n",
      "test Epoch:200\t KLD_Gaussian Loss: 0.833311, NLL Loss: -30.598404, Loss: -29.7651\n",
      "train Epoch:201\t KLD_Gaussian Loss: 1.135231, NLL Loss: -28.424663, Loss: -27.2894\n",
      "valid Epoch:201\t KLD_Gaussian Loss: 1.134625, NLL Loss: -28.407081, Loss: -27.2725\n",
      "test Epoch:201\t KLD_Gaussian Loss: 0.834262, NLL Loss: -30.598878, Loss: -29.7646\n",
      "train Epoch:202\t KLD_Gaussian Loss: 1.135096, NLL Loss: -28.417389, Loss: -27.2823\n",
      "valid Epoch:202\t KLD_Gaussian Loss: 1.136582, NLL Loss: -28.411826, Loss: -27.2752\n",
      "test Epoch:202\t KLD_Gaussian Loss: 0.832688, NLL Loss: -30.605402, Loss: -29.7727\n",
      "train Epoch:203\t KLD_Gaussian Loss: 1.135470, NLL Loss: -28.421680, Loss: -27.2862\n",
      "valid Epoch:203\t KLD_Gaussian Loss: 1.135913, NLL Loss: -28.405569, Loss: -27.2697\n",
      "test Epoch:203\t KLD_Gaussian Loss: 0.832779, NLL Loss: -30.599105, Loss: -29.7663\n",
      "train Epoch:204\t KLD_Gaussian Loss: 1.135577, NLL Loss: -28.419286, Loss: -27.2837\n",
      "valid Epoch:204\t KLD_Gaussian Loss: 1.133773, NLL Loss: -28.425141, Loss: -27.2914\n",
      "test Epoch:204\t KLD_Gaussian Loss: 0.829542, NLL Loss: -30.610426, Loss: -29.7809\n",
      "train Epoch:205\t KLD_Gaussian Loss: 1.136646, NLL Loss: -28.416060, Loss: -27.2794\n",
      "valid Epoch:205\t KLD_Gaussian Loss: 1.133902, NLL Loss: -28.406226, Loss: -27.2723\n",
      "test Epoch:205\t KLD_Gaussian Loss: 0.832841, NLL Loss: -30.595024, Loss: -29.7622\n",
      "train Epoch:206\t KLD_Gaussian Loss: 1.136195, NLL Loss: -28.423060, Loss: -27.2869\n",
      "valid Epoch:206\t KLD_Gaussian Loss: 1.136781, NLL Loss: -28.397699, Loss: -27.2609\n",
      "test Epoch:206\t KLD_Gaussian Loss: 0.831037, NLL Loss: -30.608877, Loss: -29.7778\n",
      "train Epoch:207\t KLD_Gaussian Loss: 1.135108, NLL Loss: -28.416681, Loss: -27.2816\n",
      "valid Epoch:207\t KLD_Gaussian Loss: 1.132259, NLL Loss: -28.414271, Loss: -27.2820\n",
      "test Epoch:207\t KLD_Gaussian Loss: 0.830815, NLL Loss: -30.596912, Loss: -29.7661\n",
      "train Epoch:208\t KLD_Gaussian Loss: 1.132750, NLL Loss: -28.427016, Loss: -27.2943\n",
      "valid Epoch:208\t KLD_Gaussian Loss: 1.132152, NLL Loss: -28.413956, Loss: -27.2818\n",
      "test Epoch:208\t KLD_Gaussian Loss: 0.834167, NLL Loss: -30.603770, Loss: -29.7696\n",
      "train Epoch:209\t KLD_Gaussian Loss: 1.135584, NLL Loss: -28.423244, Loss: -27.2877\n",
      "valid Epoch:209\t KLD_Gaussian Loss: 1.135512, NLL Loss: -28.410373, Loss: -27.2749\n",
      "test Epoch:209\t KLD_Gaussian Loss: 0.831550, NLL Loss: -30.600342, Loss: -29.7688\n",
      "train Epoch:210\t KLD_Gaussian Loss: 1.134147, NLL Loss: -28.422135, Loss: -27.2880\n",
      "valid Epoch:210\t KLD_Gaussian Loss: 1.133944, NLL Loss: -28.409450, Loss: -27.2755\n",
      "test Epoch:210\t KLD_Gaussian Loss: 0.831691, NLL Loss: -30.597969, Loss: -29.7663\n",
      "train Epoch:211\t KLD_Gaussian Loss: 1.135943, NLL Loss: -28.417946, Loss: -27.2820\n",
      "valid Epoch:211\t KLD_Gaussian Loss: 1.133523, NLL Loss: -28.416514, Loss: -27.2830\n",
      "test Epoch:211\t KLD_Gaussian Loss: 0.832309, NLL Loss: -30.597141, Loss: -29.7648\n",
      "train Epoch:212\t KLD_Gaussian Loss: 1.136259, NLL Loss: -28.418397, Loss: -27.2821\n",
      "valid Epoch:212\t KLD_Gaussian Loss: 1.134971, NLL Loss: -28.417596, Loss: -27.2826\n",
      "test Epoch:212\t KLD_Gaussian Loss: 0.832507, NLL Loss: -30.598227, Loss: -29.7657\n",
      "train Epoch:213\t KLD_Gaussian Loss: 1.134455, NLL Loss: -28.421585, Loss: -27.2871\n",
      "valid Epoch:213\t KLD_Gaussian Loss: 1.136001, NLL Loss: -28.413809, Loss: -27.2778\n",
      "test Epoch:213\t KLD_Gaussian Loss: 0.832911, NLL Loss: -30.600715, Loss: -29.7678\n",
      "train Epoch:214\t KLD_Gaussian Loss: 1.134985, NLL Loss: -28.415265, Loss: -27.2803\n",
      "valid Epoch:214\t KLD_Gaussian Loss: 1.136044, NLL Loss: -28.417505, Loss: -27.2815\n",
      "test Epoch:214\t KLD_Gaussian Loss: 0.834702, NLL Loss: -30.591783, Loss: -29.7571\n",
      "train Epoch:215\t KLD_Gaussian Loss: 1.135097, NLL Loss: -28.423261, Loss: -27.2882\n",
      "valid Epoch:215\t KLD_Gaussian Loss: 1.133225, NLL Loss: -28.400423, Loss: -27.2672\n",
      "test Epoch:215\t KLD_Gaussian Loss: 0.832236, NLL Loss: -30.605674, Loss: -29.7734\n",
      "train Epoch:216\t KLD_Gaussian Loss: 1.136288, NLL Loss: -28.424609, Loss: -27.2883\n",
      "valid Epoch:216\t KLD_Gaussian Loss: 1.136053, NLL Loss: -28.417275, Loss: -27.2812\n",
      "test Epoch:216\t KLD_Gaussian Loss: 0.832739, NLL Loss: -30.598195, Loss: -29.7655\n",
      "train Epoch:217\t KLD_Gaussian Loss: 1.134167, NLL Loss: -28.424023, Loss: -27.2899\n",
      "valid Epoch:217\t KLD_Gaussian Loss: 1.137564, NLL Loss: -28.401398, Loss: -27.2638\n",
      "test Epoch:217\t KLD_Gaussian Loss: 0.832913, NLL Loss: -30.599037, Loss: -29.7661\n",
      "train Epoch:218\t KLD_Gaussian Loss: 1.134001, NLL Loss: -28.421628, Loss: -27.2876\n",
      "valid Epoch:218\t KLD_Gaussian Loss: 1.134748, NLL Loss: -28.406772, Loss: -27.2720\n",
      "test Epoch:218\t KLD_Gaussian Loss: 0.834150, NLL Loss: -30.593805, Loss: -29.7597\n",
      "train Epoch:219\t KLD_Gaussian Loss: 1.135680, NLL Loss: -28.422357, Loss: -27.2867\n",
      "valid Epoch:219\t KLD_Gaussian Loss: 1.140026, NLL Loss: -28.399126, Loss: -27.2591\n",
      "test Epoch:219\t KLD_Gaussian Loss: 0.829312, NLL Loss: -30.618101, Loss: -29.7888\n",
      "train Epoch:220\t KLD_Gaussian Loss: 1.134884, NLL Loss: -28.420426, Loss: -27.2855\n",
      "valid Epoch:220\t KLD_Gaussian Loss: 1.136267, NLL Loss: -28.411839, Loss: -27.2756\n",
      "test Epoch:220\t KLD_Gaussian Loss: 0.835025, NLL Loss: -30.593108, Loss: -29.7581\n",
      "train Epoch:221\t KLD_Gaussian Loss: 1.135368, NLL Loss: -28.419471, Loss: -27.2841\n",
      "valid Epoch:221\t KLD_Gaussian Loss: 1.137268, NLL Loss: -28.400111, Loss: -27.2628\n",
      "test Epoch:221\t KLD_Gaussian Loss: 0.831743, NLL Loss: -30.600966, Loss: -29.7692\n",
      "train Epoch:222\t KLD_Gaussian Loss: 1.134775, NLL Loss: -28.417712, Loss: -27.2829\n",
      "valid Epoch:222\t KLD_Gaussian Loss: 1.135333, NLL Loss: -28.418877, Loss: -27.2835\n",
      "test Epoch:222\t KLD_Gaussian Loss: 0.830681, NLL Loss: -30.610793, Loss: -29.7801\n",
      "train Epoch:223\t KLD_Gaussian Loss: 1.135855, NLL Loss: -28.425657, Loss: -27.2898\n",
      "valid Epoch:223\t KLD_Gaussian Loss: 1.134091, NLL Loss: -28.417236, Loss: -27.2831\n",
      "test Epoch:223\t KLD_Gaussian Loss: 0.831503, NLL Loss: -30.597758, Loss: -29.7663\n",
      "train Epoch:224\t KLD_Gaussian Loss: 1.133762, NLL Loss: -28.420320, Loss: -27.2866\n",
      "valid Epoch:224\t KLD_Gaussian Loss: 1.135966, NLL Loss: -28.425538, Loss: -27.2896\n",
      "test Epoch:224\t KLD_Gaussian Loss: 0.830368, NLL Loss: -30.596571, Loss: -29.7662\n",
      "train Epoch:225\t KLD_Gaussian Loss: 1.134714, NLL Loss: -28.418147, Loss: -27.2834\n",
      "valid Epoch:225\t KLD_Gaussian Loss: 1.133532, NLL Loss: -28.415991, Loss: -27.2825\n",
      "test Epoch:225\t KLD_Gaussian Loss: 0.832986, NLL Loss: -30.605628, Loss: -29.7726\n",
      "train Epoch:226\t KLD_Gaussian Loss: 1.134720, NLL Loss: -28.423839, Loss: -27.2891\n",
      "valid Epoch:226\t KLD_Gaussian Loss: 1.136648, NLL Loss: -28.402227, Loss: -27.2656\n",
      "test Epoch:226\t KLD_Gaussian Loss: 0.832590, NLL Loss: -30.598026, Loss: -29.7654\n",
      "train Epoch:227\t KLD_Gaussian Loss: 1.133602, NLL Loss: -28.422296, Loss: -27.2887\n",
      "valid Epoch:227\t KLD_Gaussian Loss: 1.133475, NLL Loss: -28.414723, Loss: -27.2812\n",
      "test Epoch:227\t KLD_Gaussian Loss: 0.832634, NLL Loss: -30.602505, Loss: -29.7699\n",
      "train Epoch:228\t KLD_Gaussian Loss: 1.133619, NLL Loss: -28.426782, Loss: -27.2932\n",
      "valid Epoch:228\t KLD_Gaussian Loss: 1.135412, NLL Loss: -28.404990, Loss: -27.2696\n",
      "test Epoch:228\t KLD_Gaussian Loss: 0.831141, NLL Loss: -30.597383, Loss: -29.7662\n",
      "train Epoch:229\t KLD_Gaussian Loss: 1.135209, NLL Loss: -28.417205, Loss: -27.2820\n",
      "valid Epoch:229\t KLD_Gaussian Loss: 1.136150, NLL Loss: -28.411176, Loss: -27.2750\n",
      "test Epoch:229\t KLD_Gaussian Loss: 0.831105, NLL Loss: -30.603895, Loss: -29.7728\n",
      "train Epoch:230\t KLD_Gaussian Loss: 1.132926, NLL Loss: -28.423790, Loss: -27.2909\n",
      "valid Epoch:230\t KLD_Gaussian Loss: 1.136806, NLL Loss: -28.406928, Loss: -27.2701\n",
      "test Epoch:230\t KLD_Gaussian Loss: 0.831851, NLL Loss: -30.606969, Loss: -29.7751\n",
      "train Epoch:231\t KLD_Gaussian Loss: 1.134870, NLL Loss: -28.419244, Loss: -27.2844\n",
      "valid Epoch:231\t KLD_Gaussian Loss: 1.137908, NLL Loss: -28.406534, Loss: -27.2686\n",
      "test Epoch:231\t KLD_Gaussian Loss: 0.829989, NLL Loss: -30.606141, Loss: -29.7762\n",
      "train Epoch:232\t KLD_Gaussian Loss: 1.134709, NLL Loss: -28.427722, Loss: -27.2930\n",
      "valid Epoch:232\t KLD_Gaussian Loss: 1.134696, NLL Loss: -28.415600, Loss: -27.2809\n",
      "test Epoch:232\t KLD_Gaussian Loss: 0.833302, NLL Loss: -30.601857, Loss: -29.7686\n",
      "train Epoch:233\t KLD_Gaussian Loss: 1.136673, NLL Loss: -28.422480, Loss: -27.2858\n",
      "valid Epoch:233\t KLD_Gaussian Loss: 1.135593, NLL Loss: -28.415786, Loss: -27.2802\n",
      "test Epoch:233\t KLD_Gaussian Loss: 0.829322, NLL Loss: -30.610605, Loss: -29.7813\n",
      "train Epoch:234\t KLD_Gaussian Loss: 1.133736, NLL Loss: -28.430823, Loss: -27.2971\n",
      "valid Epoch:234\t KLD_Gaussian Loss: 1.134045, NLL Loss: -28.412610, Loss: -27.2786\n",
      "test Epoch:234\t KLD_Gaussian Loss: 0.829991, NLL Loss: -30.613145, Loss: -29.7832\n",
      "train Epoch:235\t KLD_Gaussian Loss: 1.136126, NLL Loss: -28.420164, Loss: -27.2840\n",
      "valid Epoch:235\t KLD_Gaussian Loss: 1.135852, NLL Loss: -28.400693, Loss: -27.2648\n",
      "test Epoch:235\t KLD_Gaussian Loss: 0.830857, NLL Loss: -30.605587, Loss: -29.7747\n",
      "train Epoch:236\t KLD_Gaussian Loss: 1.135388, NLL Loss: -28.420094, Loss: -27.2847\n",
      "valid Epoch:236\t KLD_Gaussian Loss: 1.136418, NLL Loss: -28.398863, Loss: -27.2624\n",
      "test Epoch:236\t KLD_Gaussian Loss: 0.831137, NLL Loss: -30.612362, Loss: -29.7812\n",
      "train Epoch:237\t KLD_Gaussian Loss: 1.135287, NLL Loss: -28.424439, Loss: -27.2892\n",
      "valid Epoch:237\t KLD_Gaussian Loss: 1.134770, NLL Loss: -28.409252, Loss: -27.2745\n",
      "test Epoch:237\t KLD_Gaussian Loss: 0.832561, NLL Loss: -30.598140, Loss: -29.7656\n",
      "train Epoch:238\t KLD_Gaussian Loss: 1.135404, NLL Loss: -28.420830, Loss: -27.2854\n",
      "valid Epoch:238\t KLD_Gaussian Loss: 1.136339, NLL Loss: -28.395879, Loss: -27.2595\n",
      "test Epoch:238\t KLD_Gaussian Loss: 0.830316, NLL Loss: -30.600872, Loss: -29.7706\n",
      "train Epoch:239\t KLD_Gaussian Loss: 1.136054, NLL Loss: -28.418054, Loss: -27.2820\n",
      "valid Epoch:239\t KLD_Gaussian Loss: 1.136789, NLL Loss: -28.412002, Loss: -27.2752\n",
      "test Epoch:239\t KLD_Gaussian Loss: 0.833214, NLL Loss: -30.598388, Loss: -29.7652\n",
      "train Epoch:240\t KLD_Gaussian Loss: 1.135756, NLL Loss: -28.421109, Loss: -27.2854\n",
      "valid Epoch:240\t KLD_Gaussian Loss: 1.136042, NLL Loss: -28.408933, Loss: -27.2729\n",
      "test Epoch:240\t KLD_Gaussian Loss: 0.831906, NLL Loss: -30.605841, Loss: -29.7739\n",
      "train Epoch:241\t KLD_Gaussian Loss: 1.135503, NLL Loss: -28.422860, Loss: -27.2874\n",
      "valid Epoch:241\t KLD_Gaussian Loss: 1.135371, NLL Loss: -28.415253, Loss: -27.2799\n",
      "test Epoch:241\t KLD_Gaussian Loss: 0.832387, NLL Loss: -30.598195, Loss: -29.7658\n",
      "train Epoch:242\t KLD_Gaussian Loss: 1.135316, NLL Loss: -28.420200, Loss: -27.2849\n",
      "valid Epoch:242\t KLD_Gaussian Loss: 1.138544, NLL Loss: -28.403921, Loss: -27.2654\n",
      "test Epoch:242\t KLD_Gaussian Loss: 0.832697, NLL Loss: -30.608931, Loss: -29.7762\n",
      "train Epoch:243\t KLD_Gaussian Loss: 1.135995, NLL Loss: -28.418040, Loss: -27.2820\n",
      "valid Epoch:243\t KLD_Gaussian Loss: 1.136500, NLL Loss: -28.401649, Loss: -27.2651\n",
      "test Epoch:243\t KLD_Gaussian Loss: 0.831574, NLL Loss: -30.601053, Loss: -29.7695\n",
      "train Epoch:244\t KLD_Gaussian Loss: 1.134475, NLL Loss: -28.421630, Loss: -27.2872\n",
      "valid Epoch:244\t KLD_Gaussian Loss: 1.138367, NLL Loss: -28.413156, Loss: -27.2748\n",
      "test Epoch:244\t KLD_Gaussian Loss: 0.832893, NLL Loss: -30.603642, Loss: -29.7707\n",
      "train Epoch:245\t KLD_Gaussian Loss: 1.136451, NLL Loss: -28.422326, Loss: -27.2859\n",
      "valid Epoch:245\t KLD_Gaussian Loss: 1.133803, NLL Loss: -28.414983, Loss: -27.2812\n",
      "test Epoch:245\t KLD_Gaussian Loss: 0.831792, NLL Loss: -30.607048, Loss: -29.7753\n",
      "train Epoch:246\t KLD_Gaussian Loss: 1.134129, NLL Loss: -28.423983, Loss: -27.2899\n",
      "valid Epoch:246\t KLD_Gaussian Loss: 1.136132, NLL Loss: -28.413969, Loss: -27.2778\n",
      "test Epoch:246\t KLD_Gaussian Loss: 0.832755, NLL Loss: -30.603736, Loss: -29.7710\n",
      "train Epoch:247\t KLD_Gaussian Loss: 1.133614, NLL Loss: -28.432289, Loss: -27.2987\n",
      "valid Epoch:247\t KLD_Gaussian Loss: 1.136088, NLL Loss: -28.414859, Loss: -27.2788\n",
      "test Epoch:247\t KLD_Gaussian Loss: 0.834908, NLL Loss: -30.594204, Loss: -29.7593\n",
      "train Epoch:248\t KLD_Gaussian Loss: 1.134782, NLL Loss: -28.422633, Loss: -27.2879\n",
      "valid Epoch:248\t KLD_Gaussian Loss: 1.135088, NLL Loss: -28.398265, Loss: -27.2632\n",
      "test Epoch:248\t KLD_Gaussian Loss: 0.832872, NLL Loss: -30.594243, Loss: -29.7614\n",
      "train Epoch:249\t KLD_Gaussian Loss: 1.135616, NLL Loss: -28.421493, Loss: -27.2859\n",
      "valid Epoch:249\t KLD_Gaussian Loss: 1.139355, NLL Loss: -28.407974, Loss: -27.2686\n",
      "test Epoch:249\t KLD_Gaussian Loss: 0.831596, NLL Loss: -30.601573, Loss: -29.7700\n",
      "train Epoch:250\t KLD_Gaussian Loss: 1.132803, NLL Loss: -28.420509, Loss: -27.2877\n",
      "valid Epoch:250\t KLD_Gaussian Loss: 1.135082, NLL Loss: -28.413205, Loss: -27.2781\n",
      "test Epoch:250\t KLD_Gaussian Loss: 0.831926, NLL Loss: -30.598378, Loss: -29.7665\n",
      "train Epoch:251\t KLD_Gaussian Loss: 1.133628, NLL Loss: -28.426107, Loss: -27.2925\n",
      "valid Epoch:251\t KLD_Gaussian Loss: 1.136506, NLL Loss: -28.410910, Loss: -27.2744\n",
      "test Epoch:251\t KLD_Gaussian Loss: 0.831542, NLL Loss: -30.610980, Loss: -29.7794\n",
      "train Epoch:252\t KLD_Gaussian Loss: 1.134747, NLL Loss: -28.408958, Loss: -27.2742\n",
      "valid Epoch:252\t KLD_Gaussian Loss: 1.135449, NLL Loss: -28.420350, Loss: -27.2849\n",
      "test Epoch:252\t KLD_Gaussian Loss: 0.833692, NLL Loss: -30.591114, Loss: -29.7574\n",
      "train Epoch:253\t KLD_Gaussian Loss: 1.135648, NLL Loss: -28.417866, Loss: -27.2822\n",
      "valid Epoch:253\t KLD_Gaussian Loss: 1.134805, NLL Loss: -28.406655, Loss: -27.2718\n",
      "test Epoch:253\t KLD_Gaussian Loss: 0.833335, NLL Loss: -30.589523, Loss: -29.7562\n",
      "train Epoch:254\t KLD_Gaussian Loss: 1.134294, NLL Loss: -28.420061, Loss: -27.2858\n",
      "valid Epoch:254\t KLD_Gaussian Loss: 1.135105, NLL Loss: -28.404392, Loss: -27.2693\n",
      "test Epoch:254\t KLD_Gaussian Loss: 0.833800, NLL Loss: -30.590333, Loss: -29.7565\n",
      "train Epoch:255\t KLD_Gaussian Loss: 1.134372, NLL Loss: -28.425388, Loss: -27.2910\n",
      "valid Epoch:255\t KLD_Gaussian Loss: 1.137410, NLL Loss: -28.402435, Loss: -27.2650\n",
      "test Epoch:255\t KLD_Gaussian Loss: 0.830136, NLL Loss: -30.607443, Loss: -29.7773\n",
      "train Epoch:256\t KLD_Gaussian Loss: 1.134780, NLL Loss: -28.417021, Loss: -27.2822\n",
      "valid Epoch:256\t KLD_Gaussian Loss: 1.137365, NLL Loss: -28.404529, Loss: -27.2672\n",
      "test Epoch:256\t KLD_Gaussian Loss: 0.830073, NLL Loss: -30.604806, Loss: -29.7747\n",
      "train Epoch:257\t KLD_Gaussian Loss: 1.134551, NLL Loss: -28.420429, Loss: -27.2859\n",
      "valid Epoch:257\t KLD_Gaussian Loss: 1.131896, NLL Loss: -28.418110, Loss: -27.2862\n",
      "test Epoch:257\t KLD_Gaussian Loss: 0.834423, NLL Loss: -30.592679, Loss: -29.7583\n",
      "train Epoch:258\t KLD_Gaussian Loss: 1.136412, NLL Loss: -28.425539, Loss: -27.2891\n",
      "valid Epoch:258\t KLD_Gaussian Loss: 1.134953, NLL Loss: -28.400817, Loss: -27.2659\n",
      "test Epoch:258\t KLD_Gaussian Loss: 0.833906, NLL Loss: -30.591521, Loss: -29.7576\n",
      "train Epoch:259\t KLD_Gaussian Loss: 1.134382, NLL Loss: -28.419806, Loss: -27.2854\n",
      "valid Epoch:259\t KLD_Gaussian Loss: 1.137458, NLL Loss: -28.398070, Loss: -27.2606\n",
      "test Epoch:259\t KLD_Gaussian Loss: 0.832098, NLL Loss: -30.594998, Loss: -29.7629\n",
      "train Epoch:260\t KLD_Gaussian Loss: 1.134677, NLL Loss: -28.417290, Loss: -27.2826\n",
      "valid Epoch:260\t KLD_Gaussian Loss: 1.135431, NLL Loss: -28.411631, Loss: -27.2762\n",
      "test Epoch:260\t KLD_Gaussian Loss: 0.829403, NLL Loss: -30.606665, Loss: -29.7773\n",
      "train Epoch:261\t KLD_Gaussian Loss: 1.133416, NLL Loss: -28.423662, Loss: -27.2902\n",
      "valid Epoch:261\t KLD_Gaussian Loss: 1.132585, NLL Loss: -28.410204, Loss: -27.2776\n",
      "test Epoch:261\t KLD_Gaussian Loss: 0.834482, NLL Loss: -30.602344, Loss: -29.7679\n",
      "train Epoch:262\t KLD_Gaussian Loss: 1.134740, NLL Loss: -28.424953, Loss: -27.2902\n",
      "valid Epoch:262\t KLD_Gaussian Loss: 1.133900, NLL Loss: -28.411079, Loss: -27.2772\n",
      "test Epoch:262\t KLD_Gaussian Loss: 0.831827, NLL Loss: -30.589158, Loss: -29.7573\n",
      "train Epoch:263\t KLD_Gaussian Loss: 1.135348, NLL Loss: -28.421847, Loss: -27.2865\n",
      "valid Epoch:263\t KLD_Gaussian Loss: 1.136679, NLL Loss: -28.417964, Loss: -27.2813\n",
      "test Epoch:263\t KLD_Gaussian Loss: 0.834660, NLL Loss: -30.581240, Loss: -29.7466\n",
      "train Epoch:264\t KLD_Gaussian Loss: 1.135584, NLL Loss: -28.427103, Loss: -27.2915\n",
      "valid Epoch:264\t KLD_Gaussian Loss: 1.132222, NLL Loss: -28.409905, Loss: -27.2777\n",
      "test Epoch:264\t KLD_Gaussian Loss: 0.832781, NLL Loss: -30.599730, Loss: -29.7669\n",
      "train Epoch:265\t KLD_Gaussian Loss: 1.133667, NLL Loss: -28.423839, Loss: -27.2902\n",
      "valid Epoch:265\t KLD_Gaussian Loss: 1.135263, NLL Loss: -28.410035, Loss: -27.2748\n",
      "test Epoch:265\t KLD_Gaussian Loss: 0.832789, NLL Loss: -30.596317, Loss: -29.7635\n",
      "train Epoch:266\t KLD_Gaussian Loss: 1.133711, NLL Loss: -28.424559, Loss: -27.2908\n",
      "valid Epoch:266\t KLD_Gaussian Loss: 1.136393, NLL Loss: -28.409821, Loss: -27.2734\n",
      "test Epoch:266\t KLD_Gaussian Loss: 0.829270, NLL Loss: -30.604459, Loss: -29.7752\n",
      "train Epoch:267\t KLD_Gaussian Loss: 1.135306, NLL Loss: -28.421382, Loss: -27.2861\n",
      "valid Epoch:267\t KLD_Gaussian Loss: 1.132399, NLL Loss: -28.415165, Loss: -27.2828\n",
      "test Epoch:267\t KLD_Gaussian Loss: 0.832116, NLL Loss: -30.598741, Loss: -29.7666\n",
      "train Epoch:268\t KLD_Gaussian Loss: 1.135776, NLL Loss: -28.420202, Loss: -27.2844\n",
      "valid Epoch:268\t KLD_Gaussian Loss: 1.133808, NLL Loss: -28.407750, Loss: -27.2739\n",
      "test Epoch:268\t KLD_Gaussian Loss: 0.832594, NLL Loss: -30.605301, Loss: -29.7727\n",
      "train Epoch:269\t KLD_Gaussian Loss: 1.135313, NLL Loss: -28.419261, Loss: -27.2839\n",
      "valid Epoch:269\t KLD_Gaussian Loss: 1.135022, NLL Loss: -28.417834, Loss: -27.2828\n",
      "test Epoch:269\t KLD_Gaussian Loss: 0.833612, NLL Loss: -30.600350, Loss: -29.7667\n",
      "train Epoch:270\t KLD_Gaussian Loss: 1.135855, NLL Loss: -28.418434, Loss: -27.2826\n",
      "valid Epoch:270\t KLD_Gaussian Loss: 1.139273, NLL Loss: -28.405088, Loss: -27.2658\n",
      "test Epoch:270\t KLD_Gaussian Loss: 0.830153, NLL Loss: -30.597004, Loss: -29.7669\n",
      "train Epoch:271\t KLD_Gaussian Loss: 1.135238, NLL Loss: -28.420009, Loss: -27.2848\n",
      "valid Epoch:271\t KLD_Gaussian Loss: 1.136797, NLL Loss: -28.418016, Loss: -27.2812\n",
      "test Epoch:271\t KLD_Gaussian Loss: 0.830681, NLL Loss: -30.602804, Loss: -29.7721\n",
      "train Epoch:272\t KLD_Gaussian Loss: 1.133998, NLL Loss: -28.422407, Loss: -27.2884\n",
      "valid Epoch:272\t KLD_Gaussian Loss: 1.140989, NLL Loss: -28.400352, Loss: -27.2594\n",
      "test Epoch:272\t KLD_Gaussian Loss: 0.831148, NLL Loss: -30.606963, Loss: -29.7758\n",
      "train Epoch:273\t KLD_Gaussian Loss: 1.135938, NLL Loss: -28.422147, Loss: -27.2862\n",
      "valid Epoch:273\t KLD_Gaussian Loss: 1.135894, NLL Loss: -28.417798, Loss: -27.2819\n",
      "test Epoch:273\t KLD_Gaussian Loss: 0.830490, NLL Loss: -30.605527, Loss: -29.7750\n",
      "train Epoch:274\t KLD_Gaussian Loss: 1.135355, NLL Loss: -28.421489, Loss: -27.2861\n",
      "valid Epoch:274\t KLD_Gaussian Loss: 1.135992, NLL Loss: -28.414931, Loss: -27.2789\n",
      "test Epoch:274\t KLD_Gaussian Loss: 0.832234, NLL Loss: -30.590721, Loss: -29.7585\n",
      "train Epoch:275\t KLD_Gaussian Loss: 1.136051, NLL Loss: -28.419275, Loss: -27.2832\n",
      "valid Epoch:275\t KLD_Gaussian Loss: 1.137217, NLL Loss: -28.408023, Loss: -27.2708\n",
      "test Epoch:275\t KLD_Gaussian Loss: 0.830749, NLL Loss: -30.600487, Loss: -29.7697\n",
      "train Epoch:276\t KLD_Gaussian Loss: 1.134154, NLL Loss: -28.423211, Loss: -27.2891\n",
      "valid Epoch:276\t KLD_Gaussian Loss: 1.134483, NLL Loss: -28.418786, Loss: -27.2843\n",
      "test Epoch:276\t KLD_Gaussian Loss: 0.828741, NLL Loss: -30.604284, Loss: -29.7755\n",
      "train Epoch:277\t KLD_Gaussian Loss: 1.134923, NLL Loss: -28.424248, Loss: -27.2893\n",
      "valid Epoch:277\t KLD_Gaussian Loss: 1.137456, NLL Loss: -28.407090, Loss: -27.2696\n",
      "test Epoch:277\t KLD_Gaussian Loss: 0.830478, NLL Loss: -30.596626, Loss: -29.7661\n",
      "train Epoch:278\t KLD_Gaussian Loss: 1.135566, NLL Loss: -28.415654, Loss: -27.2801\n",
      "valid Epoch:278\t KLD_Gaussian Loss: 1.137545, NLL Loss: -28.417873, Loss: -27.2803\n",
      "test Epoch:278\t KLD_Gaussian Loss: 0.832346, NLL Loss: -30.592059, Loss: -29.7597\n",
      "train Epoch:279\t KLD_Gaussian Loss: 1.133226, NLL Loss: -28.427316, Loss: -27.2941\n",
      "valid Epoch:279\t KLD_Gaussian Loss: 1.136181, NLL Loss: -28.398070, Loss: -27.2619\n",
      "test Epoch:279\t KLD_Gaussian Loss: 0.831811, NLL Loss: -30.592863, Loss: -29.7611\n",
      "train Epoch:280\t KLD_Gaussian Loss: 1.134032, NLL Loss: -28.425026, Loss: -27.2910\n",
      "valid Epoch:280\t KLD_Gaussian Loss: 1.136143, NLL Loss: -28.415607, Loss: -27.2795\n",
      "test Epoch:280\t KLD_Gaussian Loss: 0.829962, NLL Loss: -30.600135, Loss: -29.7702\n",
      "train Epoch:281\t KLD_Gaussian Loss: 1.134013, NLL Loss: -28.425362, Loss: -27.2913\n",
      "valid Epoch:281\t KLD_Gaussian Loss: 1.136638, NLL Loss: -28.400257, Loss: -27.2636\n",
      "test Epoch:281\t KLD_Gaussian Loss: 0.831206, NLL Loss: -30.590395, Loss: -29.7592\n",
      "train Epoch:282\t KLD_Gaussian Loss: 1.135649, NLL Loss: -28.424691, Loss: -27.2890\n",
      "valid Epoch:282\t KLD_Gaussian Loss: 1.137174, NLL Loss: -28.414157, Loss: -27.2770\n",
      "test Epoch:282\t KLD_Gaussian Loss: 0.831877, NLL Loss: -30.595316, Loss: -29.7634\n",
      "train Epoch:283\t KLD_Gaussian Loss: 1.135596, NLL Loss: -28.420870, Loss: -27.2853\n",
      "valid Epoch:283\t KLD_Gaussian Loss: 1.136052, NLL Loss: -28.408599, Loss: -27.2725\n",
      "test Epoch:283\t KLD_Gaussian Loss: 0.831390, NLL Loss: -30.606012, Loss: -29.7746\n",
      "train Epoch:284\t KLD_Gaussian Loss: 1.135701, NLL Loss: -28.419511, Loss: -27.2838\n",
      "valid Epoch:284\t KLD_Gaussian Loss: 1.135138, NLL Loss: -28.414398, Loss: -27.2793\n",
      "test Epoch:284\t KLD_Gaussian Loss: 0.831794, NLL Loss: -30.599440, Loss: -29.7676\n",
      "train Epoch:285\t KLD_Gaussian Loss: 1.134495, NLL Loss: -28.420299, Loss: -27.2858\n",
      "valid Epoch:285\t KLD_Gaussian Loss: 1.134008, NLL Loss: -28.412525, Loss: -27.2785\n",
      "test Epoch:285\t KLD_Gaussian Loss: 0.832758, NLL Loss: -30.594392, Loss: -29.7616\n",
      "train Epoch:286\t KLD_Gaussian Loss: 1.134669, NLL Loss: -28.417254, Loss: -27.2826\n",
      "valid Epoch:286\t KLD_Gaussian Loss: 1.138341, NLL Loss: -28.401424, Loss: -27.2631\n",
      "test Epoch:286\t KLD_Gaussian Loss: 0.829868, NLL Loss: -30.604010, Loss: -29.7741\n",
      "train Epoch:287\t KLD_Gaussian Loss: 1.135985, NLL Loss: -28.421932, Loss: -27.2859\n",
      "valid Epoch:287\t KLD_Gaussian Loss: 1.134810, NLL Loss: -28.407389, Loss: -27.2726\n",
      "test Epoch:287\t KLD_Gaussian Loss: 0.832129, NLL Loss: -30.599907, Loss: -29.7678\n",
      "train Epoch:288\t KLD_Gaussian Loss: 1.135887, NLL Loss: -28.417184, Loss: -27.2813\n",
      "valid Epoch:288\t KLD_Gaussian Loss: 1.136259, NLL Loss: -28.410474, Loss: -27.2742\n",
      "test Epoch:288\t KLD_Gaussian Loss: 0.832314, NLL Loss: -30.599933, Loss: -29.7676\n",
      "train Epoch:289\t KLD_Gaussian Loss: 1.134867, NLL Loss: -28.420792, Loss: -27.2859\n",
      "valid Epoch:289\t KLD_Gaussian Loss: 1.133215, NLL Loss: -28.409603, Loss: -27.2764\n",
      "test Epoch:289\t KLD_Gaussian Loss: 0.831288, NLL Loss: -30.605297, Loss: -29.7740\n",
      "train Epoch:290\t KLD_Gaussian Loss: 1.135514, NLL Loss: -28.413891, Loss: -27.2784\n",
      "valid Epoch:290\t KLD_Gaussian Loss: 1.134405, NLL Loss: -28.405481, Loss: -27.2711\n",
      "test Epoch:290\t KLD_Gaussian Loss: 0.831522, NLL Loss: -30.610480, Loss: -29.7790\n",
      "train Epoch:291\t KLD_Gaussian Loss: 1.134718, NLL Loss: -28.421151, Loss: -27.2864\n",
      "valid Epoch:291\t KLD_Gaussian Loss: 1.137798, NLL Loss: -28.407341, Loss: -27.2695\n",
      "test Epoch:291\t KLD_Gaussian Loss: 0.832601, NLL Loss: -30.601621, Loss: -29.7690\n",
      "train Epoch:292\t KLD_Gaussian Loss: 1.134616, NLL Loss: -28.415661, Loss: -27.2810\n",
      "valid Epoch:292\t KLD_Gaussian Loss: 1.137080, NLL Loss: -28.407302, Loss: -27.2702\n",
      "test Epoch:292\t KLD_Gaussian Loss: 0.832995, NLL Loss: -30.602135, Loss: -29.7691\n",
      "train Epoch:293\t KLD_Gaussian Loss: 1.134823, NLL Loss: -28.429065, Loss: -27.2942\n",
      "valid Epoch:293\t KLD_Gaussian Loss: 1.135437, NLL Loss: -28.410237, Loss: -27.2748\n",
      "test Epoch:293\t KLD_Gaussian Loss: 0.833486, NLL Loss: -30.592921, Loss: -29.7594\n",
      "train Epoch:294\t KLD_Gaussian Loss: 1.135367, NLL Loss: -28.417155, Loss: -27.2818\n",
      "valid Epoch:294\t KLD_Gaussian Loss: 1.134351, NLL Loss: -28.405767, Loss: -27.2714\n",
      "test Epoch:294\t KLD_Gaussian Loss: 0.830705, NLL Loss: -30.594718, Loss: -29.7640\n",
      "train Epoch:295\t KLD_Gaussian Loss: 1.134568, NLL Loss: -28.418744, Loss: -27.2842\n",
      "valid Epoch:295\t KLD_Gaussian Loss: 1.132741, NLL Loss: -28.415743, Loss: -27.2830\n",
      "test Epoch:295\t KLD_Gaussian Loss: 0.833688, NLL Loss: -30.599365, Loss: -29.7657\n",
      "train Epoch:296\t KLD_Gaussian Loss: 1.134609, NLL Loss: -28.423384, Loss: -27.2888\n",
      "valid Epoch:296\t KLD_Gaussian Loss: 1.136835, NLL Loss: -28.413699, Loss: -27.2769\n",
      "test Epoch:296\t KLD_Gaussian Loss: 0.832788, NLL Loss: -30.602640, Loss: -29.7699\n",
      "train Epoch:297\t KLD_Gaussian Loss: 1.136597, NLL Loss: -28.418583, Loss: -27.2820\n",
      "valid Epoch:297\t KLD_Gaussian Loss: 1.135043, NLL Loss: -28.407682, Loss: -27.2726\n",
      "test Epoch:297\t KLD_Gaussian Loss: 0.833568, NLL Loss: -30.600171, Loss: -29.7666\n",
      "train Epoch:298\t KLD_Gaussian Loss: 1.136153, NLL Loss: -28.420592, Loss: -27.2844\n",
      "valid Epoch:298\t KLD_Gaussian Loss: 1.133292, NLL Loss: -28.414742, Loss: -27.2815\n",
      "test Epoch:298\t KLD_Gaussian Loss: 0.832611, NLL Loss: -30.602582, Loss: -29.7700\n",
      "train Epoch:299\t KLD_Gaussian Loss: 1.134532, NLL Loss: -28.414337, Loss: -27.2798\n",
      "valid Epoch:299\t KLD_Gaussian Loss: 1.135591, NLL Loss: -28.403089, Loss: -27.2675\n",
      "test Epoch:299\t KLD_Gaussian Loss: 0.833111, NLL Loss: -30.599081, Loss: -29.7660\n",
      "train Epoch:300\t KLD_Gaussian Loss: 1.136729, NLL Loss: -28.414819, Loss: -27.2781\n",
      "valid Epoch:300\t KLD_Gaussian Loss: 1.135560, NLL Loss: -28.411791, Loss: -27.2762\n",
      "test Epoch:300\t KLD_Gaussian Loss: 0.832901, NLL Loss: -30.597377, Loss: -29.7645\n",
      "Running Time: 3200.1755409240723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#writer = SummaryWriter()\n",
    "## Training\n",
    "start = time.time()\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "loss_test_list = []\n",
    "best_validation = 1e5\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    ### Training\n",
    "    loss_train,(all_z_posterior_mean_train,all_z_posterior_std_train),(all_z_prior_mean_train, all_z_prior_std_train),all_z_t_sampled_train, all_y_emission_mean_train, all_y_emission_std_train, all_h_train = train(model,optimizer,trainY,epoch,batch_size)\n",
    "    \n",
    "    ### Validation\n",
    "    loss_valid,(all_z_posterior_mean_valid,all_z_posterior_std_valid),(all_z_prior_mean_valid, all_z_prior_std_valid),all_z_t_sampled_valid, all_y_emission_mean_valid, all_y_emission_std_valid, all_h_valid  = test(model,validY,epoch,\"valid\")\n",
    "    \n",
    "    ### Testing\n",
    "    loss_test,(all_z_posterior_mean_test,all_z_posterior_std_test),(all_z_prior_mean_test, all_z_prior_std_test),all_z_t_sampled_test, all_y_emission_mean_test, all_y_emission_std_test, all_h_test = test(model,testY,epoch,\"test\")\n",
    "    \n",
    "    ### Learning rate adjust\n",
    "    scheduler.step(loss_valid)\n",
    "    \n",
    "    loss_train_list.append(loss_train)\n",
    "    loss_valid_list.append(loss_valid)\n",
    "    loss_test_list.append(loss_test)\n",
    "    \n",
    "    ### Save the results to tensorboard\n",
    "#     writer.add_scalar(\"scalar/train_loss\",loss_train,epoch)\n",
    "#     writer.add_scalar(\"scalar/valid_loss\",loss_valid,epoch)\n",
    "    \n",
    "    ### Save checkpoint\n",
    "    if (epoch % save_every == 0):\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss': loss_train,\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(epoch, 'checkpoint')))\n",
    "        \n",
    "    if save_best:\n",
    "        \n",
    "        if not os.path.exists(directoryBest):\n",
    "            os.makedirs(directoryBest)\n",
    "        if (loss_valid < best_validation):\n",
    "            best_validation = copy.deepcopy(loss_valid)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss': loss_train,\n",
    "            },os.path.join(directoryBest,'best.tar'))\n",
    "\n",
    "\n",
    "print(\"Running Time:\", time.time()-start)\n",
    "plt.plot(np.array(loss_train_list),label=\"train\")\n",
    "plt.plot(np.array(loss_valid_list),label=\"validation\")\n",
    "plt.plot(np.array(loss_test_list),label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "#writer.add_graph(model,trainY[:,0:1,:])\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time: 3369.197920560837\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt81NWd+P/Xe+4zmczkRkIIhAQBCTcBI+h6V6RardrWqlvdrrZbel3b3a2tLlvrfrfdXy+uP7dbqz+7a39upbWurrrdagWt1isoICAIckdIgNwvk0zmer5/zCQETACZMJ/J5P18PPLIzPl85nPen/kk75ycOZ9zxBiDUkqp/GezOgCllFLZoQlfKaXGCE34Sik1RmjCV0qpMUITvlJKjRGa8JVSaozQhK+UUmOEJnyllBojNOErpdQY4bA6gMHKyspMTU2N1WEopdSosnbt2hZjzLjj7ZdTCb+mpoY1a9ZYHYZSSo0qIrL3RPbLqEtHRH4iIltFZKOIPCUiRenyy0RkrYi8m/5+SSb1KKWUylymffgrgdnGmLnANuDOdHkL8AljzBzgL4FfZViPUkqpDGWU8I0xK4wx8fTTVcDEdPk7xpjGdPlmwCsi7kzqUkoplZmR7MP/PPDbIco/DawzxkRGsC6l1CgQi8XYv38/fX19VoeSFzweDxMnTsTpdJ7U64+b8EXkBWD8EJuWGWOeSe+zDIgDy4967SzgR8CSYxx/KbAUoLq6+oQDV0rlvv3791NYWEhNTQ0iYnU4o5oxhtbWVvbv309tbe1JHeO4Cd8Ys/hY20XkFuAq4FIzaDUVEZkIPAV8zhiz8xjHfwh4CKC+vl5XY1Eqj/T19WmyHyEiQmlpKc3NzSd9jIy6dETkcuDbwIXGmN5B5UXA74E7jDGvZ1KHUmp002Q/cjJ9LzMdpfMzoBBYKSLrReTBdPnXganAXeny9SJSnmFdw+vcD3/8AaZlxymrQimlRrtMR+lMNcZMMsbMS399OV3+fWNMwaDyecaYppEJ+cPe37UbXvkxmze+daqqUEqNQh0dHfz85z//yK/7+Mc/TkdHxymIyFp5MZeO01cEQKwn/y6QUurkDZfw4/H4EHsf9uyzz1JUVHSqwrJMTk2tcLK8gWIAkuFOiyNRSuWSO+64g507dzJv3jycTicej4fi4mK2bt3Ktm3buPbaa9m3bx99fX184xvfYOnSpcDhaV5CoRBXXHEF5513Hm+88QZVVVU888wzeL1ei8/s5ORFwi8IlABgwl0WR6KUGs4//m4z7zWO7O/ozAkBvveJWcNu/+EPf8imTZtYv349L7/8MldeeSWbNm0aGNb48MMPU1JSQjgc5qyzzuLTn/40paWlRxxj+/bt/OY3v+EXv/gF119/PU8++SQ333zziJ5HtuRHwvd6CRsXRDThK6WGt3DhwiPGsP/0pz/lqaeeAmDfvn1s3779Qwm/traWefPmAXDmmWeyZ8+erMU70vIi4dttQggfEtWEr1SuOlZLPFsKCgoGHr/88su88MILvPnmm/h8Pi666KIh7wh2uw/PCmO32wmHw1mJ9VTIiw9tAXptBdij3VaHoZTKIYWFhXR3D50XOjs7KS4uxufzsXXrVlatWpXl6LIvL1r4AL1SgDMWsjoMpVQOKS0t5dxzz2X27Nl4vV4qKioGtl1++eU8+OCD1NXVcfrpp3P22WdbGGl25E3Cj9gLKEhowldKHenXv/71kOVut5vnnntuyG39/fRlZWVs2rRpoPxb3/rWiMeXTXnTpRN1+PFowldKqWHlTcKPOQvxJjXhK6XUcPIm4SddhRQcnr9NKaXUUfIq4XuJQCJmdShKKZWT8ibh4w4CEOvV6RWUUmooeZPwxRsAINzdZnEkSimVm/Im4du9qRZ+b1e7xZEopUYrv98PQGNjI9ddd92Q+1x00UWsWbPmmMe577776O09/Jlirky3nDcJ31mQmsq0L2T9m6qUGt0mTJjAE088cdKvPzrh58p0y3mU8FNTJMdC2qWjlEq54447uP/++wee33333Xz/+9/n0ksvZcGCBcyZM4dnnnnmQ6/bs2cPs2fPBiAcDnPjjTdSV1fHJz/5ySPm0vnKV75CfX09s2bN4nvf+x6QmpCtsbGRiy++mIsvvhhITbfc0tICwL333svs2bOZPXs2991330B9dXV1fPGLX2TWrFksWbLklMzZk+matj8BPgFEgZ3ArcaYDhFZSHphckCAu40xT2UU6XG4guMBiHefsoW1lFKZeO4OOPjuyB5z/By44ofDbr7hhhv45je/yde+9jUAHn/8cZ5//nluu+02AoEALS0tnH322Vx99dXDrhf7wAMP4PP52LJlCxs3bmTBggUD237wgx9QUlJCIpHg0ksvZePGjdx2223ce++9vPTSS5SVlR1xrLVr1/LLX/6S1atXY4xh0aJFXHjhhRQXF2dlGuZMW/grgdnGmLnANuDOdPkmoN4YMw+4HPj/ROSUTuMQKK0kaYRE14FTWY1SahSZP38+TU1NNDY2smHDBoqLixk/fjx///d/z9y5c1m8eDENDQ0cOnRo2GO88sorA4l37ty5zJ07d2Db448/zoIFC5g/fz6bN2/mvffeO2Y8r732Gp/85CcpKCjA7/fzqU99ildffRXIzjTMGSVhY8yKQU9XAdelywffAeUBTCb1nIhxRX7aKMR0D3/hlFIWOkZL/FT6zGc+wxNPPMHBgwe54YYbWL58Oc3Nzaxduxan00lNTc2Q0yIfz+7du7nnnnt4++23KS4u5pZbbjmp4/TLxjTMI9mH/3lgYCYiEVkkIpuBd4EvG2OOvYhkhjxOO21ShCPcfCqrUUqNMjfccAOPPfYYTzzxBJ/5zGfo7OykvLwcp9PJSy+9xN69e4/5+gsuuGBgArZNmzaxceNGALq6uigoKCAYDHLo0KEjJmIbblrm888/n6effpre3l56enp46qmnOP/880fwbI/tuC18EXkBGD/EpmXGmGfS+ywD4sDy/o3GmNXALBGpAx4RkeeMMR/68yciS4GlANXV1Sd1Ev267SWU9GnCV0odNmvWLLq7u6mqqqKyspKbbrqJT3ziE8yZM4f6+npmzJhxzNd/5Stf4dZbb6Wuro66ujrOPPNMAM444wzmz5/PjBkzmDRpEueee+7Aa5YuXcrll1/OhAkTeOmllwbKFyxYwC233MLChQsB+Ku/+ivmz5+ftVW0xJjMeltE5BbgS8ClR3XlDN7nj8C3jTHHHLxaX19vjje+9Vhe+dGnqYtsYNxdO076GEqpkbNlyxbq6uqsDiOvDPWeishaY0z98V6bUZeOiFwOfBu4enCyF5Ha/g9pRWQyMAPYk0ldJyLmHUcw2Q4Z/hFTSql8lGkf/s+AQmCliKwXkQfT5ecBG0RkPfAU8FVjTEuGdR1X0l+BizgmrHfbKqXU0TIdpTN1mPJfAb/K5Ngnw16YWr4s1NpIoa8k29UrpVROy5s7bQFcRZUAdDXvtzgSpZTKPXmV8H1lkwAIN+22OBKllMo9eZXwi6umEzFOEk1brA5FKaVyTl4l/EmlfnYxAUfr+1aHopTKAR0dHfz85z8/qdcePeNlPsirhO+w2zjgqiEY2mV1KEqpHKAJ/0indEIzK/QEp1HW8ieIdIO70OpwlFIWuuOOO9i5cyfz5s3jsssuo7y8nMcff5xIJMInP/lJ/vEf/5Genh6uv/569u/fTyKR4Lvf/S6HDh0amOK4rKzsiLtlR7O8S/gybga0QF/jZjy1Z1sdjlIq7Udv/YitbVtH9JgzSmbwnYXfGXb7D3/4QzZt2sT69etZsWIFTzzxBG+99RbGGK6++mpeeeUVmpubmTBhAr///e8B6OzsJBgMDjvF8WiWV106AP7qMwBo3f62xZEopXLJihUrWLFiBfPnz2fBggVs3bqV7du3M2fOHFauXMl3vvMdXn31VYLBoNWhnjJ518KvPq2OBlNKYtefgL+2OhylVNqxWuLZYIzhzjvv5Etf+tKHtq1bt45nn32Wf/iHf+DSSy/lrrvusiDCUy/vWvi14/yss82lpHk1JJNWh6OUstDgaYo/9rGP8fDDDxMKhQBoaGgYWBzF5/Nx8803c/vtt7Nu3boPvTZf5F0LX0ToqjwHf+NLJA9sxFY1z+qQlFIWKS0t5dxzz2X27NlcccUVfPazn+Wcc84BwO/38+ijj7Jjxw5uv/12bDYbTqeTBx54ABh+iuPRLOPpkUdSptMj9/vd6+9wxYpL6JpzKyXX3TsCkSmlToZOjzzyLJseOVctnFPH/yTPxb/5UQjpgihKKQV5mvArAh72zvwy9mSU7mfz88MXpZT6qPIy4QN89srFPMJVFL73a8zbD+uiKEpZJJe6jUe7TN/LvEn4ffE+YsnYwPOKgAffx/6BNxMzkd//Dcn/vAYObdaRO0plkcfjobW1VZP+CDDG0NraisfjOelj5MWHtuub1vMXz/0FDy5+kHOrDi8kbIzhn363mcjq/+Dvnb+hgDAA8ZJpOMZNB385+CvS38vBVQA2Bzh94A6AJwDxCCTjYHeBww12Z+qx3ZXaV2TEzl+pfBOLxdi/fz99fX1Wh5IXPB4PEydOxOl0HlF+oh/aZjQsU0R+AnwCiAI7gVuNMR2DtlcD7wF3G2PuyaSuYwm4AwB0RDqOKBcR7rp6Ni9Ou4O/ePZSzmj7A8XSzczmvUxue5cKWyeBZGdGdSdtLozdCTYnxukFVyG4CxFvAFugEpm0CKZcBMU1GdWj1GjkdDqpra21OgyVluk4/JXAncaYuIj8CLgTGHw73b3AcxnWcVxF7iLgwwm/36V1FVwy40o27D+Ptp4Iu1t6+cOBLrYe7KKtq4cSunH1NeMyUSQZw0OEQnoJSC9RHESME5fEcRHHOejLLUc+9xLFL70UEqZQ9lEtayld/2uSYiex8Ms4P/ZPYLOf6rdDKaWGlOmatisGPV0FXNf/RESuBXYDPZnUcSICrlQLvzMyfGtdRJg3qei4x0omDaFonHjicFdXPJkkGk8Siae+H/E4kSASSxJNJInEkrQlkhyMJ4nEE/xXWy/7tm/kiu4n+ezq++ns7iR4/f2Zn7BSSp2EkbzT9vPAbwFExE+qpX8Z8K0RrGNIDpuDQmfhMRP+ibLZhIDHefwdT9hcVu+6gl/96pv8xXuPEt55E97T/mwEj6+UUifmuKN0ROQFEdk0xNc1g/ZZBsSB5emiu4H/1xgTOoHjLxWRNSKyprn55G+SCrqDw3bpWG3RlFJmffb/oc34aXn2B1aHo5Qao47bwjfGLD7WdhG5BbgKuNQcHvKzCLhORH4MFAFJEekzxvxsiOM/BDwEqVE6Hy38w4rcRSPSwj9VFkybyH8XXsWnWh8jGe7E5s3fKViVUrkpo3H4InI58G3gamPMwFpgxpjzjTE1xpga4D7gn4dK9iMp6MndFn6/cVNSE7m1HdAlGJVS2ZfpjVc/AwqBlSKyXkQeHIGYTkqRuyjnE763PDU8rb1RE75SKvsyHaUz9QT2uTuTOk5UrnfpABRVngZAuHmPtYEopcakvJlaIegOEoqFjpheIdeMr5pM1NiJt31gdShKqTEofxK+K/UhaC638v0eF4ekDEd3g9WhKKXGoLxJ+P1323ZFuiyO5NjaHOV4w41Wh6GUGoPyLuHn+ge3IU8lRdFDVoehlBqD8ibhBz2pLp32SLvFkRxb3F9FSbIVE49aHYpSaozJm4Rf6ikFoK2vzeJIjs0WrMQuhlCHtvKVUtmVNwm/xFMCQFs4txO+s7AMgK6WgxZHopQaa/Im4bvsLgpdhbT2tVodyjG5g+UA9LRrwldKZVfeJHxIdeu0hnM74fuLxwMQ7miyOBKl1FiTVwm/xFOS8334gdJUwo91a8JXSmVXXiX8Um9pznfpFJVWkDRCMtRidShKqTEmvxL+KOjScbtcdOBHenM7TqVU/smrhF/iLaEr2kUskbvz6QB024I4crzrSSmVf/Iq4Y+Wsfg9jiLc0dy+QUwplX/yK+F7Uwk/1/vxw85iCuKa8JVS2ZVfCT/dws/1fvy4p4TCZO7O6qmUyk95lfDHF6SGPB7oOWBxJMeW9JYSNN0kEwmrQ1FKjSGZrmn7ExHZKiIbReQpESlKl9eISDi97GHWlj4s95XjdXjZ3bk7G9WdNPGXYRdDV3uz1aEopcaQTFv4K4HZxpi5wDbgzkHbdhpj5qW/vpxhPSfEJjZqAjU5n/Ad/tR8Op2tuf2fiFIqv2SU8I0xK4wx8fTTVcDEzEPKTG2wNucTvqeoAoCeNp1PRymVPSPZh/954LlBz2tF5B0R+ZOInD+C9RxTbbCWxp5GwvFwtqr8yAqKUwk/3KnTKyilssdxvB1E5AVg/BCblhljnknvswyIA8vT2w4A1caYVhE5E3haRGYZYz60/qCILAWWAlRXV5/cWQxSG6wFYG/XXmaUzMj4eKdCoKQSgFiXJnylVPYcN+EbYxYfa7uI3AJcBVxqjDHp10SASPrxWhHZCUwH1gxx/IeAhwDq6+vNR4z/Q6YEpwCwvX17zib8YGmqha/z6SilsinTUTqXA98GrjbG9A4qHyci9vTjKcA0YFcmdZ2o2mAtBc4CNjRvyEZ1J8Xh9tKNF3Q+HaVUFh23hX8cPwPcwEoRAViVHpFzAfB/RCQGJIEvG2OyMt+Bw+Zgbtlc3ml6JxvVnbRuCeKI5PYUEEqp/JJRwjfGTB2m/EngyUyOnYn5FfN5YP0DdEW7CLgCVoVxTCFHEe4cX3BdKZVf8upO234LyhdgMKw7tM7qUIYVcRXpfDpKqazKy4Q/r3weJZ4SHn//catDGVbUXarz6SilsiovE77b7uamupt4teFV3m973+pwhmS8pRSZLuJxnU9HKZUdeZnwAW44/QYKXYXcs+Ye0qNFc4oUlOGWOO2d2q2jlMqOvE34QXeQr8/7OqsOrOLhTQ/nXNK3F6bm0wnp9ApKqSzJ24QPcP3p13PZ5Mu4b919fPf179IX77M6pAGuwnIAetoOWRyJUmqsyHQcfk5z2Bzcc+E9PLjhQR7Y8ACvNbzGpMJJnFZ0GosqF7Fw/MKBVbKyzVOUSviRTk34SqnsyOuED6kpk78676ssHL+Q5VuW0xntZMWeFTy5PXWbQLk3lXh9Th/xZByX3UWZtwyf04dDHDhsDuw2+8Bjh82B0+bEZXcd8d1hc2AXOw6bA5fdhcfuocxbxhnjzsBpd34orsKS1PREsW6dE18plR15n/D71Y+vp358PQDxZJytbVtZfWA1uzp3YRc7oVgIh81BPBmnubeZzkgnCZMgnowTT8aPeBxNRokmosSSsePW63V4Oa/qPL4w+wvMKps1UF5Ymkr4Op+OUipbxkzCH8xhczC7bDazy2ZndBxjDPFknFgyRiwZG/ijEE1EiSQi7O3ayxuNb/Dc7ud4Ye8L3DzzZm6vvx0RweML0GecOp+OUiprxmTCHykigtPuHLLLBuC0otO4pPoSvrngm9y79l5+9d6vmFk6k6umXAUidEoAR58mfKVUduT1KJ1c4Xf5WbZoGXPK5nDP2/cQT6YWCeu2F+GK6jh8pVR2aMLPErvNzrVTr6W1r5WWcKrfvtdZhC+mCV8plR2a8LNofEHqg9pDvamhmBFnMf6EzqejlMoOTfhZVOFLrXR1sCd1d23cW0rQaMJXSmWHJvwsGmjh96Ra+ElvKQX0EYv0HutlSik1IjThZ1HAFcDr8A506dgKUvPpdLY0WhmWUmqMyHRN25+IyFYR2SgiT4lI0aBtc0XkTRHZLCLviogn83BHNxGhwlcxkPBdxRMA6Dy0z8qwlFJjRKYt/JXAbGPMXGAbcCeAiDiAR0mtZTsLuAg4/m2pY0CFr2KgD7+grBqAnhZN+EqpUy+jhG+MWWGMiaefrgImph8vATYaYzak92s1xuhKH0BFweEWflHlZAAibZrwlVKn3kj24X8eeC79eDpgROR5EVknIt8ewXpGtQpfBc29zSSSCcrKKokYJ3RpH75S6tQ77tQKIvICMH6ITcuMMc+k91kGxIHlg457HnAW0Au8KCJrjTEvDnH8pcBSgOrq6pM5h1GlzFtGwiTojHZS4inhkBTj6NFFUJRSp95xE74xZvGxtovILcBVwKXm8LJS+4FXjDEt6X2eBRYAH0r4xpiHgIcA6uvrc2tZqlMg4A4A0BXposRTQodjHJ6+JoujUkqNBZmO0rkc+DZwtTFm8GDy54E5IuJLf4B7IfBeJnXli6ArCEBnNHXDVY97HIGYzomvlDr1Mu3D/xlQCKwUkfUi8iCAMaYduBd4G1gPrDPG/D7DuvJC0J1O+JFUwo/6KihJtkKOrbmrlMo/GU2PbIyZeoxtj5IamqkGCbjSXTrRrlRBYSXe5ih93a14AmUWRqaUynd6p22WHd3Ct1XMBODgljcti0kpNTZows+yQlchcLiFf9qZi4kYB52bn7cyLKXUGKAJP8scNgd+p5+uSCrhV5SVssVRR9GB1y2OTCmV7zThWyDoDg506QC0VvwZk2O7iHQcsDAqpVS+04RvgYArcPhDWyA4awkAO1Y/a1VISqkxQBO+BQLuwBEt/DlnXUCnKaBnywsWRqWUynea8C0QdAUHbrwCcLtc7AnUU92xmmhM55hTSp0amvAtEHQHBz607eeesZjxtLLxpd9aFJVSKt9pwrdAwBWgM9qJGXR37bTFf8V2qWHGm7dD+17rglNK5S1N+BYIuoPEk3HC8fBAmd3tY9VZ95FMJmn95Y2YPl3cXCk1sjThW6D/btvBI3UAPnXp+TxcfgfBzq20/MvZvPOLr9K69XV4+UfQ02JFqEqpPKIJ3wL98+kMHqkDUOB28NdfuY0nZv2cDyIFzNz/W0of+zi8/M9E/+NKTOM7VoSrlMoTmvAtcPR8OoPZbcKN1/85s+9aTcvn/sQLJZ/lW/Gv0tf6AfLQRex4/sFsh6uUyhMZzZapTs6HZswcgtthp+q02VTd9gBzuvp4Zt2fM/PVr3HGm3fS8+bdHCqcSfmVd+GfcVGWolZKjXaa8C1wrBb+UCoCHv7iorlsr1nOM8vvJOgy1HWtxv/YNXS5KoictoRx590KExaAyKkMXSk1imnCt8BAH370o43EmVZTzbRlqWWD1+5o4OnH/4UpfZu4+L3HYMuv6PVOID71YxRe8FVk3PQRj1spNbppwreA1+HFYXN86Oarj+LMqVWc+ff3EorE+e9VW2h44zHmhd7k/I2Pknj3P2mdtISyS76Ovfa8EYxcKTWaZZTwReQnwCeAKLATuNUY0yEiNwG3D9p1LrDAGLM+k/ryhYh8aHqFk+V3O/jshXNInj+bNXvbeWrPbuTVe7jsg1exP/IcLRXnUXj+l3EXFMH42eAtHoEzUEqNRpm28FcCdxpj4iLyI+BO4DvGmOXAcgARmQM8rcn+SEdPkZwpm01YWFvCwtoSoucv59m1O9n9h3/lLw8+hfuJmwHothfjvOwfkM79OHxB7GfcCIHKEYtBKZXbMl3TdsWgp6uA64bY7c+BxzKpJx8dPUXySHI5bFy7aBqRM/+VdbvuZN3rK7FHuzmn4WHO+MPfkTCCXQy8eDetFefSPvNmimdeTOk4Tf5K5bOR7MP/PDDUzF83ANeMYD15IegO0tTbdErrcDvsnDO9inOm3wLAO3tv4jdrXybmHceOxmbK9j7HzQefZeqh1+l5ycP75ZdQ2NeAvaSW8pt/gThcqQN1NUKkG8adfkrjVUqdWsdN+CLyAjB+iE3LjDHPpPdZBsRJd+MMeu0ioNcYs+kYx18KLAWorq4+8chHuaA7yPb27Vmtc/7kEuZP/tTA80j8GtbvPIg5sJ7e1x6g7tAqeoyHqV0baP3+S4RtftzVZ+Ld/xrOZBjH0j9iHz8zqzErpUbOcRO+MWbxsbaLyC3AVcClZvD0jyk3Ar85zvEfAh4CqK+vP/r1eetUdumcKLfDzqLTq+D0KpIXfJzeWIKgwIon76f44Gt0dnZx9u4V9OEijJNxD55DzO4lFJhKQ82niRk7k/yGkuqZxLFhb3kf+6Ivgt1p6XkppYaW6Sidy4FvAxcaY3qP2mYDrgfOz6SOfBVwBwjFQsSSMZw26xOkzSb43akfhyV//g3gG7T1RHln5wdUBl28+s579Kx/EnckxKWt65jdfvcRr093/rDvlf+f3rI5eBPd+GZdgS0WJrTjNQrOuIZEbwfjFt2AeIIQ6wVXQepFyQS0bINxM468cayrEfzjwaYzgCg1EjLtw/8Z4AZWSuoXdZUx5svpbRcA+4wxuzKsIy8FXam7bbuj3ZR4SiyOZmglBS7OnzsVgKmTq+Hay+nui3GgvYf3O/Zht9vY3BQn2bCOwmQHLaEYZzU8woR9vyeMm7IDzwPgM048Db8HoPfl7xHzlBDsa+CDooXYy07DveclyuIH+WDGF/AUjSfS2409GWXCpgfpLJxKh3cyJVf9HwqrZ1v1ViiVFzIdpTP1GNteBs7O5Pj5LOBOz6cT6crZhD+UQo+TwsoiqCwCYOp0gDMGtieTt9PSE6GrN8KqDW/R1dXJtFln0vb+mySdXuLv/BZfTxMHHQv4s7Y1lLRvZKNtBltsEzl/638cUdcriTkUdPYxo+s1mh7/awr/7o86dYRSGdA7bS3S38IfiZuvconNJpQXeigv9DB1yWWHN9TVApC84loaOsIsLvayvz3Mc7ta+djs8ThNnNXPP0x3xSIKSiqxd31ATe0ckgjP/teP+cyh++j5xZV4SiZgL55Mn6uIqH8ijkQEu9OJ3cSxR7vAW0K7r4beUCflu5/GKQYm/xnS0wQOD0ysT30/tBk8RTDlQggdgmhPqovJ6YNEFAITIB5JrT6WiKZeU1wDNjv0daY+p+jvkoJUt1TjO3DwXSgcD0WTwVcKiQgEJx3+Q9V1AGwOKCg7XBbthXB7qlurqBpKpqS2RXshGgJfGWAg0gUNa6F1J1TOg+pFYExqBJXDA/G+1H6eYKp8959Sx51yMXiLIB5NxQMQ6wOTSMXiLYZ9b6WOP3UxiC3V5WZ3g31QioiFoW0XBKpg/xrwBKB5K1Sdmep+qzkPnN7Uvok4HNgA5TNS76+rEOLh1HtXOjW1XyIO+1ZD9wEgCriFAAAZYUlEQVQoPS11Xv3n7wlC03up9ytQCeUzU++5MdC4LnXO4+eCuxCDkIhHcYQOpOKO9kJhBbj8sPuV1OumLk69R/3vuTGpfY1JXcd4JPV+NL8P/vJUXNMvT70HPc2pa9rxQep93PsGVJ6ROveK2an3y1uS+jnZ82rqZ6ewMnXcSHdqLYv+99rmSP08OFwQ7kjt07kfbHZMYCJyirsvNeFbpL+FP5I3X40GNpswqcQHwKQS38BjcLLo2q8N2rNq4FHsmm/w7w9sZ9H+zRQ1bKNSWvGQxDNMHSXpr17jJoyD4Dv/OWw8cRw4iH+ovMVWiivZR4Cew3GIiz7xUphMXbOouIiJm4OUMs60EjDdQ9bR5KyiyTGeZDTM3MR7ALQRwE2ciLgpMe1H7B8RD0328VTG9+EgQZOtHH+yGx/hI/ZLYAcMdpLEcWAjgQ1DXJwkEVwmmt7PRocEKTYd2DhyXEQMB81SwgSTGiLcLkW4bEkKEl3EsXNQynESJ0AITBIvEZLIh44D0CEBuh2ltNlKqIrupsy0kcCGneQR+0XFTZ948Jg+XCYy5HuWxIZt0OuiOFOjxiSKJ9FzxL69eDHG4Je+Qa8XYuLCfdTxkwgGQWDg+CHx4zM9RHHh4fD+veLDYeK4iNJrK8CXPLLeI84JJ2BwDfGzdLRevBixUWB6iOIYeM36wouZ/3dPH/f1mdCEb5H+Fr7VI3VGg2kTSrDf9hAb9newu7mH/a1dTC8yVHOIsHhIJuJEcRCiAHeklck04vN6eYcZdFGAs20bnUkvAXuMksh+4pEw+5MljO99nwnJgxxyTyZkvJQ7evAQoS8uTI7twO10sTcwn8ZeG71d7UyM76XE3ss2JlAZcGIPt2GivUz1dtFgn8nOggW83DOZMumikib8iS5MMsG82HqKoh0EbHEeSN6Ar7CIOts+QkkX9kQfm3qDJDylOMqm4OlpINC1nRrbId4vXkQzJdSF19HpKKXJPYmD7ims6a1gduhNShPN+L1O+mx+nNEO+nDRFrHhT3RT5LHR6p3CtkQ5F/IOZbYudkeLSdg9uO1Cj3Hhczuo6tvBRHOA/wx+kVAkwazet2mJ2Aj5JhCQMNW2ZmLGQTsFeB3CpvgkqhINbHLPw0cfLY5Kpie2EXYWs6DnVYiEGJdoY7d3Fo/KfCbFPmB7opwJfjstUQfi8lEZeo8Ce5zWPmG/fy6RoqlMjGynyV7OJJpxJ7rx9LXQ4qqktWAa/nAD1X3v44z30ByGnbbJmMAkauO7IB6mytFFaYGLd8x0DoTtxG1uFtq34Yy08QfXEipdfdREttDbF8UuBocYRISIzYdHYoxLttDtLCVoutnnnU5JpIHtjtOZ1buaHny0EaQ6vpudjqlMdHTxQmgyM2Qv+51TmGI/iN3hosy0Y7PBTt88JNJFma0bW6wHuyfIwbgfsds50B6izAOzzHZ6Y4ZuzwRKTDs9rjJcHh/jKk/9sHT58EhK69TX15s1a9ZYHUZWtPe1c8FvL+COhXdwU91NVoej8kT/77Pk6Wcd8UQSEcFuy8/zO1kistYYU3+8/bSFb5FCVyGgLXw1svI10fdz2HWIbib03bOIw+bA7/RnNEWyUkp9FJrwLTTSM2YqpdSxaMK3UC5Mr6CUGjs04Vso4A5oC18plTWa8C00UqteKaXUidCEb6GAO6Af2iqlskYTvoX6W/i5dC+EUip/acK3UNAdJJ6ME46Hj7+zUkplSBO+hQKu9IyZOlJHKZUFmvAtFHSnZ8zUkTpKqSzQhG+h/ha+JnylVDZowrdQfwtfu3SUUtmQUcIXkZ+IyFYR2SgiT4lIUbrcKSKPiMi7IrJFRO4cmXDzi3bpKKWyKdMW/kpgtjFmLrAN6E/snwHcxpg5wJnAl0SkJsO68s5Al47efKWUyoKMEr4xZoUxpn+Jl1XAxP5NQIGIOAAvEAW03+IoXocXh82hN18ppbJiJPvwPw88l378BNADHAA+AO4xxrSNYF15QUR0egWlVNYcdwEUEXkBGD/EpmXGmGfS+ywD4sDy9LaFQAKYABQDr4rIC8aYXUMcfymwFKC6+tQv8ZVrdAI1pVS2HDfhG2MWH2u7iNwCXAVcag7PEfBZ4A/GmBjQJCKvA/XAhxK+MeYh4CFILXH4kaLPA0FXUEfpKKWyItNROpcD3wauNsb0Dtr0AXBJep8C4GxgayZ15augO6h9+EqprMi0D/9nQCGwUkTWi8iD6fL7Ab+IbAbeBn5pjNmYYV15KeDSLh2lVHZktIi5MWbqMOUhUkMz1XEE3dqlo5TKDr3T1mIBd4BQLEQsGbM6FKVUntOEb7H+m6+6o90WR6KUynea8C1W7isH4GDPQYsjUUrlO034FqsJ1ACwp3OPpXEopfKfJnyLVQeqEYQ9XXusDkUplec04VvMbXczwT9BW/hKqVNOE34OqAnWaAtfKXXKacLPAbWBWvZ07SFpklaHopTKY5rwc0BNoIZwPExjqNHqUJRSeUwTfg44s+JMAN5ofMPiSJRS+UwTfg44reg0Jvon8vK+l60ORSmVxzTh5wAR4aJJF7H6wGp6Y73Hf4FSSp0ETfg5YvHkxUSTUZ7f87zVoSil8pQm/ByxoHwBtcFantj2hNWhKKXylCb8HCEifGb6Z9jYspGndzxtdThKqTykCT+HXH/69ZxdeTZ3vX4Xd79xN0ueWMKG5g1Wh6WUyhOa8HOI2+7m3y75NxZVLuLJ7U9yqPcQd7xyB+197VaHppTKA5muafsTEdkqIhtF5CkRKUqXu0TklyLyrohsEJGLRiTaMcDj8PBvl/wb/3rxv/LvS/6dpt4mrvvddTyy+RFaw61Wh6eUGsUybeGvBGYbY+YC24A70+VfBDDGzAEuA/5FRPS/iRPkcXi4pPoSzhp/Fo9+/FHG+8Zzz5p7WPLEEtY3ref5Pc/rgilKqY8soyRsjFlhjImnn64CJqYfzwT+mN6nCegA6jOpa6yqK61j+ZXLeerqp/A4PPzNy3/Dt/70LW774230xfusDk8pNYqMZKv788Bz6ccbgKtFxCEitcCZwKShXiQiS0VkjYisaW5uHsFw8svU4qlcM/UaWsItjPOOY82hNVz51JXc/OzNPLjhQULRkNUhKqVynBhjjr2DyAvA+CE2LTPGPJPeZxmpFvynjDFGRBzAT4CLgb2AE3jIGHPM8Yb19fVmzZo1H/0sxogPuj7gc899jh9e8EPsYueXm35JZ7STjc0bqSyo5Jqp1zCnbA7nVJ6D0+60OlylVJaIyFpjzHF7UY6b8E+goluALwGXGmOGnBdARN4A/soY896xjqUJ/+Ssb1rPj9/+MZtbN5M0SRaOX8g/n/fPVBRUWB2aUioLspLwReRy4F7gQmNM86ByX/rYPSJyGfBdY8wFxzueJvzM9MZ6eXb3s3x/1fdJmASnF5/O1+d/nYsmXWR1aEqpUyhbCX8H4Ab6xwuuMsZ8WURqgOeBJNAAfMEYs/d4x9OEPzJ2d+7mpX0v8budv2NX5y4WjV9EkbuIq6dezXlV51kdnlJqhGWtS2ckacIfWb2xXn789o/Z3r6dxp5GWsOtTC+eDqRG/1w79VrmjZuH3Wa3OFKlVCY04asjhONh7n/nfvZ27SVhEqw5tIZwPEyZt4xLJl3C9adfz+klp1sdplLqJGjCV8fUE+vh1f2vsmLvCl5reI1EMsE43zi8Di/fWfgdzq482+oQlVInSBO+OmFtfW0se20Z0USUQ72HaOhuYGbZTJLJJJ+a/ikEoSfWw+dmfg4RsTpcpdRRNOGrkxKKhvjbl/+Wg70H8dg9bGnbMrDtyilXctOMm5gzbg4AxhgMBpvOmqGUpTThq4wZY3hp30v0xnvZ3r6dR997lGgyyrxx8/C7/Gxs3ojH7uETp32C5nAzdy68k3gyTtAd1P8ElMoiTfhqxPXEenhs62O8+MGLhONh5o6by1sH3mJ/aD+C4LQ5iSaj+J1+ZpfN5q5z7mJ7+3ae3vE0iycvptBZyHlV553wXcCxZAyHOPSPh1LHoQlfZUVXtIv93ftpDDXy7O5nmVM2Z+BxV7QLAJfNRTQZBaDKX8WNp99IQ6iBPV17qPJXsb19Ozs7d1JXUkcoFmLh+IUkTZJfb/01pxWdRm+sl3869584a/xZVp6qUjlLE76y1L7uffzvzv/F7/Lz6WmfZk/XHpp6m/j5+p+zpW0LXoeXyYHJNHQ3MLV4KrXBWra0bqHAWcA7Te+QMAkunnQxB3sOsqVtC7fOupW/rf9bq09LqZykCV/lJGMMDaEGxheMx2FzDLlPX7yPSCJC0B0E4Ib/vYFCVyH/vuTfsxmqUqPGiSZ8HV6hskpEmFg4cdhkD6kFYPqTPUBdSR1b27aSS40TpUYjTfgq580omUFnpJODPQetDkWpUU0Tvsp5daV1ACzfspy9XXsxxtDU28T6pvW8sv8VOiOdw762M9KpK4MplTb8/9VK5Yi6kjrOGn8Wj7z3CI+89whBd/CIJF/kLmJKcAqtfa1U+CrY372ftr42ij3FNPU2Uegq5KopV9ESbiFhEiypWcLcsrm47W5KvaUWnplS2aUf2qpRY0/nHt46+BbvtrxLbbCW6cXTsYmN5VuW0xvrpcRTwr7ufZT7yqkN1tLU28T4gvHs7NjJm41v4nf5cdldA11DDnFQW1RLobOQCl8FPqePJZOXsK97H9s7tlNZUInH4aEr2oXP4WNb+zbeb3ufKUVTOHfCuYRiIdr72lk8eTG7OnYRioWo8leRMAl2dOygMdTIpMJJVBdW0xPvIRQNUeIpobKgkiJ3EVvatrCpdRMl7hLmjJtDc28zrze+zoSCCXgdXjwOD/FknM2tmylwFlDqLR04z3gyjtfhxe/y47a7Oa3oNEo8Jbx14C0O9BzAaXPitDtx2pw4bA6cNid98T66ol0YDIWuQorcRQiC1+FlX/c+aoI17O/ej9/pJ2mSJElijCFhErT1tdEYaqS+op7WvlaiiSihWIjzq87nxQ9eZGrRVOpK63i3+V3iyThlvjJaw614HV76En0EXUEqCipw2900hBqYVjSNKn8V+7r34ba7Wde0jqRJEo6HKfGUEI6HCbqD2MRGT6xn4H6M99vfp72vnSp/FUXuIoo9xezt2svG5o0kTILJgclcXnM5aw+tZVPLJlx2F9OLp3Na0WmsObSG1QdWE4qFuLzmcgqcBdQEamgJtxBwBWgON+OwOSj3lVPmLeNA6ABvHXyLmaUzqfJXsaNjB7s6d+G0OZlUOIndnbspdBUyu2w2nZFO9nXvY1rxNKKJKNFElHJfORP8E9jXvY/OSCfbO7aDgVJv6cA1dNgcJE2SzkgnEwsnckn1JSf1u6GjdJQaJJaMYcOGiPDC3hdo7WtlX/c+9nXt41DvIbqiXbT1tRGOhwHwO/2EYkeuE1zgLGDeuHlsadtCW1/bcesc6hhHc4iDuIkPPC90FhKKhTAc/r0s9ZQSSUQIxUI4bU5iydhHOfUR47a7iSQiA88FOSLObBAEv8tPd7T7iPIqfxUuu4u9XXtJmiQAFb4KYsnYwLWyiY2ZJTOJmzhb27aeUH12sZMwiYHnPoePeDJONBnFY/cQTUYH6jsepy11w+Fw1++yyZdx70X3ntCxjnaiCV+7dNSY0P/LBrCkZsmQ+4SiId5rfQ+f08es0ln0xHpSrVN3kHA8jM/hw2FzEEvG2Ne9D5/DRzQRZdWBVZwx7gwCrgAHeg5gExu1wVqC7iDtfe2pVrPLT6GrkNZwKwd6DtDW1zbQKu6OdrOxeSOlnlLqSusGPnPoS/RhExvF7mJEhEQygU1sdEY6cdld9MZ76Yv3EY6H2dK2hZ5YD6cXn86MkhnETZxYIkYsGSOejBNLxnDb3RS5iwBo7WulO9pNd7Sb9kg7U4JTaAw1Mr14OuF4GBHBLvaBP5I+pw+vw8u29m1U+atw2900hhr5r23/xS2zbiEUS71388bNo8BZwKHeQ5T7ygnHw3gdXjoiHRzqOUQkEaHcVz7wR3NCwQTC8TCnl5xOsacYn8NHU28TPoeP1r5W7GIn4A6QNEkSJsGEggkUe4ppCbcQjoXZ270Xn8PHgooFQGrxn/fb3qcmWMOMkhkA7OzYSUOogXnl8wi4Ahhj6Ip20RvrZW/3Xip8FXRHuyn3lZMwCZp6m2gNt+Jz+jiz4kz2dO7hUO8hpgSnDPwHdyB0gEp/JdFElM2tmyn1lFLpr2Rr21YCrgBuu5uDPQdpCDVQ5a+i1FvKBP8EXDYXoViItr621M9SIobdZifoDuJ3+k/lrwCgLXyllBr1sjYOX0T+SUQ2ish6EVkhIhPS5SIiPxWRHentCzKtSyml1MkbiWGZPzHGzDXGzAP+F7grXX4FMC39tRR4YATqUkopdZIyTvjGmK5BTwtg4FOca4D/NCmrgCIRqcy0PqWUUidnRD60FZEfAJ8DOoGL08VVwL5Bu+1Plx046rVLSf0HQHV19UiEo5RSaggn1MIXkRdEZNMQX9cAGGOWGWMmAcuBr3+UAIwxDxlj6o0x9ePGjfvoZ6CUUuqEnFAL3xiz+ASPtxx4Fvge0ABMGrRtYrpMKaWUBUZilM60QU+vAfrvaPgf4HPp0TpnA53GmAMfOoBSSqmsGIk+/B+KyOlAEtgLfDld/izwcWAH0AvcOgJ1KaWUOkk5deOViDST+qNxssqAlhEKx0r5ch6g55Kr9Fxy08mey2RjzHE/BM2phJ8pEVlzIneb5bp8OQ/Qc8lVei656VSfi86Hr5RSY4QmfKWUGiPyLeE/ZHUAIyRfzgP0XHKVnktuOqXnkld9+EoppYaXby18pZRSw8iLhC8il4vI++mpmO+wOp6PSkT2iMi76Smm16TLSkRkpYhsT38vtjrOoYjIwyLSJCKbBpUNGXuuT5k9zLncLSIN6WuzXkQ+PmjbnelzeV9EPmZN1B8mIpNE5CUReU9ENovIN9Llo+66HONcRuN18YjIWyKyIX0u/5gurxWR1emYfysirnS5O/18R3p7TcZBGGNG9RdgB3YCUwAXsAGYaXVcH/Ec9gBlR5X9GLgj/fgO4EdWxzlM7BcAC4BNx4ud1I14zwECnA2stjr+EziXu4FvDbHvzPTPmhuoTf8M2q0+h3RslcCC9ONCYFs63lF3XY5xLqPxugjgTz92AqvT7/fjwI3p8geBr6QffxV4MP34RuC3mcaQDy38hcAOY8wuY0wUeIzUFA+j3TXAI+nHjwDXWhjLsIwxrwBHL/A6XOw5PWX2MOcynGuAx4wxEWPMblJ3lC88ZcF9BMaYA8aYdenH3cAWUjPVjrrrcoxzGU4uXxdjjOlf5NiZ/jLAJcAT6fKjr0v/9XoCuFREJJMY8iHhDzcN82higBUisjY9XTRAhTk899BBoMKa0E7KcLGP1mv19XRXx8ODutZGxbmkuwHmk2pNjurrctS5wCi8LiJiF5H1QBOwktR/IB3GDKxkPzjegXNJb+8ESjOpPx8Sfj44zxizgNQqYV8TkQsGbzSp/+lG5XCq0Rx72gPAacA8Ums5/Iu14Zw4EfEDTwLfNEcuVDTqrssQ5zIqr4sxJmFSqwNOJPWfx4xs1p8PCX/UT8NsjGlIf28CniL1g3Co/9/q9Pcm6yL8yIaLfdRdK2PMofQvaRL4BYe7B3L6XETESSpBLjfG/He6eFRel6HOZbRel37GmA7gJeAcUl1o/RNZDo534FzS24NAayb15kPCfxuYlv6k20Xqw43/sTimEyYiBSJS2P8YWAJsInUOf5ne7S+BZ6yJ8KQMF/uomzL7qL7sT5K6NpA6lxvTIylqSa3d/Fa24xtKup/3P4Atxph7B20adddluHMZpddlnIgUpR97gctIfSbxEnBderejr0v/9boO+GP6P7OTZ/Un1yPxRWqUwTZS/WHLrI7nI8Y+hdSogg3A5v74SfXVvQhsB14ASqyOdZj4f0PqX+oYqf7HLwwXO6lRCvenr9O7QL3V8Z/AufwqHevG9C9g5aD9l6XP5X3gCqvjHxTXeaS6azYC69NfHx+N1+UY5zIar8tc4J10zJuAu9LlU0j9UdoB/BfgTpd70s93pLdPyTQGvdNWKaXGiHzo0lFKKXUCNOErpdQYoQlfKaXGCE34Sik1RmjCV0qpMUITvhpzRCQxaJbF9TKCM6yKSM3g2TaVyiWO4++iVN4Jm9Tt7UqNKdrCVypNUusS/FhSaxO8JSJT0+U1IvLH9ERdL4pIdbq8QkSeSs9vvkFE/ix9KLuI/CI95/mK9F2VSllOE74ai7xHdencMGhbpzFmDvAz4L502b8Bjxhj5gLLgZ+my38K/MkYcwapefQ3p8unAfcbY2YBHcCnT/H5KHVC9E5bNeaISMgY4x+ifA9wiTFmV3rCroPGmFIRaSF1634sXX7AGFMmIs3ARGNMZNAxaoCVxphp6effAZzGmO+f+jNT6ti0ha/Ukcwwjz+KyKDHCfSzMpUjNOErdaQbBn1/M/34DVKzsALcBLyafvwi8BUYWNgimK0glToZ2vJQY5E3vepQvz8YY/qHZhaLyEZSrfQ/T5f9NfBLEbkdaAZuTZd/A3hIRL5AqiX/FVKzbSqVk7QPX6m0dB9+vTGmxepYlDoVtEtHKaXGCG3hK6XUGKEtfKWUGiM04Sul1BihCV8ppcYITfhKKTVGaMJXSqkxQhO+UkqNEf8X9qGbYR9VSwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Running Time:\", time.time()-start)\n",
    "plt.plot(np.array(loss_train_list),label=\"train\")\n",
    "plt.plot(np.array(loss_valid_list),label=\"validation\")\n",
    "plt.plot(np.array(loss_test_list),label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1319],\n",
       "        [1.2479]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation \n",
    "Chose the best model with best validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DSVMCode' from '/home/xiuqin/DSVM/Code/DSVMCode.py'>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(DSVMCode)\n",
    "from DSVMCode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200906MultiCheckpoint39_10/1-1-10/best'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directoryBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200906MultiCheckpoint39_10/1-1-10/best ['best.tar']\n",
      "200906MultiCheckpoint39_10/1-1-10/best/best.tar\n"
     ]
    }
   ],
   "source": [
    "# Reload the parameters\n",
    "print(directoryBest,os.listdir(directoryBest))\n",
    "PATH = os.path.join(directoryBest,os.listdir(directoryBest)[0])\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DSVM(y_dim, h_dim, z_dim,n_layers,device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204\n",
      "The total number of parameters: 911\n"
     ]
    }
   ],
   "source": [
    "## The Parameters\n",
    "print(\"Epoch:\",epoch)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "#total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"The total number of parameters:\",total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), (0.8666666666666667, 0.5176470588235295, 0.3215686274509804), (0.3333333333333333, 0.6588235294117647, 0.40784313725490196)]\n"
     ]
    }
   ],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "print(colors[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. In Sample Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the samples are put throught the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insample = torch.from_numpy(train_valid_data).float()\n",
    "# insample = torch.unsqueeze(insample,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparemodel = \"EGARCH\"\n",
    "GARCH_insample = pd.read_csv(comparemodel+\"_insampel_vol.csv\").iloc[:,1:]\n",
    "index_company = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample = torch.from_numpy(np.transpose(train_valid_Y_list2[index_company],(1,0,2))).float()\n",
    "insample.shape\n",
    "insample = insample[:,:-1,:].to(device)\n",
    "insample.shape\n",
    "\n",
    "MC = 1000\n",
    "forecaststep = 1\n",
    "\n",
    "#forecast_y,forecast_vol = model._forecastingMultiStep(outsample, S= MC,step=forecaststep)\n",
    "forecast_y,forecast_vol,forecast_z_mean,forecast_z_std,forecast_z = model._forecastingMultiStep(insample, S= MC,step=forecaststep)\n",
    "forecast_y = np.squeeze(np.squeeze(forecast_y,1),2).transpose(1,0)\n",
    "forecast_vol = np.squeeze(np.squeeze(forecast_vol,1),2).transpose(1,0)\n",
    "\n",
    "forecast_y_mean = np.mean(forecast_y,1)\n",
    "forecast_y_std = np.std(forecast_y,1)\n",
    "\n",
    "forecast_z_mean = np.squeeze(np.squeeze(forecast_z_mean,1),2).transpose(1,0)\n",
    "forecast_z_mean = np.mean(forecast_z_mean,1)    \n",
    "\n",
    "PERMNO = bigcompany_list.values[index_company][0]\n",
    "RawData = sp500_df.loc[sp500_df['PERMNO'] == PERMNO]\n",
    "RawData.reset_index(inplace=True)\n",
    "\n",
    "Date = RawData.iloc[timestep:(timestep+insample.shape[1])]['Date'].values\n",
    "price = RawData.iloc[timestep:(timestep+insample.shape[1])]['PRC'].values\n",
    "returnx = RawData.iloc[timestep:(timestep+insample.shape[1])]['RETX'].values\n",
    "GARCH_insample_vol = GARCH_insample.iloc[timestep:(timestep+insample.shape[1]),index_company].values\n",
    "\n",
    "GARCH_insample.shape\n",
    "GARCH_insample_vol.shape\n",
    "\n",
    "print(index_company,PERMNO)\n",
    "\n",
    "length = insample.shape[1] #500\n",
    "length_r = insample.shape[1]#np.random.randint(insample.shape[0])\n",
    "length_l = length_r - length\n",
    "\n",
    "## Make plot\n",
    "fig,(ax0,ax1,ax2,ax3) = plt.subplots(4,1,figsize=(20, 6),sharex = True)\n",
    "\n",
    "ax0.plot(price[length_l:length_r],label='price')\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot(returnx[length_l:length_r],label = \"true return\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(forecast_y_std[length_l:length_r],alpha=1,color ='#ff7f0e',label = \"Volatility\")\n",
    "ax2.plot(GARCH_insample_vol[length_l:length_r],alpha=1,color='green',label = \"EGARCH Volatility\")\n",
    "ax2.legend()    \n",
    "\n",
    "ax2.set_xticks(np.arange(0,length,int(length/8)))\n",
    "ax2.set_xticklabels(Date[length_l:length_r][np.arange(0,length,int(length/8))])\n",
    "\n",
    "ax3.plot(forecast_z_mean[length_l:length_r])\n",
    "\n",
    "plt.suptitle(\"In sample forecasting\")\n",
    "plt.show();\n",
    "\n",
    "\n",
    "length = 500\n",
    "length_r = np.random.randint(501,insample.shape[1])\n",
    "length_l = length_r - length\n",
    "\n",
    "## Make plot\n",
    "fig,(ax0,ax1,ax2,ax3) = plt.subplots(4,1,figsize=(20, 6),sharex = True)\n",
    "\n",
    "ax0.plot(price[length_l:length_r],label='price')\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot(returnx[length_l:length_r],label = \"true return\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(forecast_y_std[length_l:length_r],alpha=1,color ='#ff7f0e',label = \"Volatility\")\n",
    "ax2.plot(GARCH_insample_vol[length_l:length_r],alpha=1,color='green',label = \"EGARCH Volatility\")\n",
    "ax2.legend()    \n",
    "\n",
    "ax2.set_xticks(np.arange(0,length,int(length/8)))\n",
    "ax2.set_xticklabels(Date[length_l:length_r][np.arange(0,length,int(length/8))])\n",
    "\n",
    "ax3.plot(forecast_z_mean[length_l:length_r])\n",
    "\n",
    "plt.suptitle(\"In sample forecasting\")\n",
    "plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_gauss(std, x):\n",
    "    return  1/(np.sqrt(2*math.pi)*std) * np.exp(-np.power(x,2)/(2*np.power(std,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_mix_gauss(std, x):\n",
    "    return  0.5*np.log(2*math.pi) + np.log(std)+ np.power(x,2)/(2*np.power(std,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(columns = ['PERMON','NLL'])\n",
    "predicted_volatility_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_z_mean_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparemodel = \"EGARCH\"\n",
    "GARCH_outsample = pd.read_csv(comparemodel+\"_outsampel_vol.csv\").iloc[:,1:]\n",
    "for index_company in range(bigcompany_list.values.shape[0]):\n",
    "    #index_company = 0\n",
    "\n",
    "    outsample = torch.from_numpy(np.transpose(testY_list[index_company],(1,0,2))).float()\n",
    "    outsample = outsample[:,:-1,:].to(device)\n",
    "\n",
    "    MC = 1000\n",
    "    forecaststep = 1\n",
    "\n",
    "    #forecast_y,forecast_vol = model._forecastingMultiStep(outsample, S= MC,step=forecaststep)\n",
    "    forecast_y,forecast_vol,forecast_z_mean,forecast_z_std,forecast_z = model._forecastingMultiStep(outsample, S= MC,step=forecaststep)\n",
    "    \n",
    "    forecast_y = np.squeeze(np.squeeze(forecast_y,1),2).transpose(1,0)\n",
    "    forecast_vol = np.squeeze(np.squeeze(forecast_vol,1),2).transpose(1,0)\n",
    "    \n",
    "\n",
    "    forecast_y_mean = np.mean(forecast_y,1)\n",
    "    forecast_y_std = np.std(forecast_y,1)\n",
    "    \n",
    "    forecast_z_mean = np.squeeze(np.squeeze(forecast_z_mean,1),2).transpose(1,0)\n",
    "    forecast_z_mean = np.mean(forecast_z_mean,1)\n",
    "    \n",
    "    predicted_volatility_list.append(forecast_y_std)\n",
    "    \n",
    "    PERMNO = bigcompany_list.values[index_company][0]\n",
    "    RawData = sp500_df.loc[sp500_df['PERMNO'] == PERMNO]\n",
    "    RawData.reset_index(inplace=True)\n",
    "    \n",
    "    GARCH_outsample_vol = GARCH_outsample.iloc[:,index_company].values\n",
    "\n",
    "    length = 806\n",
    "    length_r = outsample.shape[1]\n",
    "    length_l = length_r - length\n",
    "\n",
    "    Date = RawData.iloc[-outsample.shape[1]:]['Date'].values[length_l:length_r]\n",
    "    price = RawData.iloc[-outsample.shape[1]:]['PRC'].values[length_l:length_r]\n",
    "    returnx = RawData.iloc[-outsample.shape[1]:]['RETX'].values[length_l:length_r]\n",
    "    \n",
    "    \n",
    "    y_mean = forecast_y_mean[length_l:length_r]\n",
    "    y_std = forecast_y_std[length_l:length_r]\n",
    "\n",
    "    fig,(ax1,ax2,ax3,ax4) = plt.subplots(4,1,figsize=(20, 8),sharex = True)\n",
    "\n",
    "    ax1.plot(price,alpha=1,label = \"price\")\n",
    "\n",
    "    ax2.plot(returnx,alpha=1,label='true return');\n",
    "    ax2.plot(y_mean,alpha=1,label='predicted return mean');\n",
    "    #ax2.plot(y_mean-y_std,alpha=1,color='grey');\n",
    "    #ax2.plot(y_mean+y_std,alpha=1,color='grey');\n",
    "    ax2.fill_between(np.arange(y_mean.shape[0]),y_mean - 1.96*y_std,y_mean + 1.96*y_std,color='grey',alpha=0.3)\n",
    "\n",
    "    ax3.plot(y_std,alpha=1,color='#ff7f0e',label='predicted volatility')\n",
    "    ax3.plot(GARCH_outsample_vol[length_l:length_r],alpha=1,color='green',label='predicted volatility(EGARCH)')\n",
    "    ax4.plot(forecast_z_mean)\n",
    "    \n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax3.legend()\n",
    "\n",
    "    # ax3.set_xticks(np.arange(0,outsample.shape[1],100))\n",
    "    # ax3.set_xticklabels(Date[np.arange(0,outsample.shape[1],100)])\n",
    "\n",
    "    ax2.set_xticks(np.arange(0,length,int(length/5)))\n",
    "    ax2.set_xticklabels(Date[np.arange(0,length,int(length/5))])\n",
    "\n",
    "    plt.suptitle(\"Recursive one-day-ahead forecasting\");\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "    #nll_test = np.mean(nll_mix_gauss(forecast_vol,np.expand_dims(returnx,1)),axis=1)\n",
    "    nll_test = -np.log(np.mean(likelihood_gauss(forecast_vol, np.expand_dims(returnx,1)),axis=1))\n",
    "    print(index_company,\"PERMON:\",bigcompany_list.values[index_company,0],\"Negative Loglikelihood is:\",np.mean(nll_test))\n",
    "    forecast_z_mean_list.append(np.mean(forecast_z_mean))\n",
    "    \n",
    "    table =  table.append({'PERMON': bigcompany_list.values[index_company,0], 'NLL': np.mean(nll_test)},ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11850.0</th>\n",
       "      <th>12060.0</th>\n",
       "      <th>12490.0</th>\n",
       "      <th>13856.0</th>\n",
       "      <th>13901.0</th>\n",
       "      <th>13928.0</th>\n",
       "      <th>14008.0</th>\n",
       "      <th>...</th>\n",
       "      <th>65875.0</th>\n",
       "      <th>66093.0</th>\n",
       "      <th>66181.0</th>\n",
       "      <th>66800.0</th>\n",
       "      <th>70519.0</th>\n",
       "      <th>76076.0</th>\n",
       "      <th>77178.0</th>\n",
       "      <th>83443.0</th>\n",
       "      <th>84788.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.021510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.011313</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.011489</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.018215</td>\n",
       "      <td>0.017688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.015943</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010203</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.016792</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011471</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.012209</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.011923</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.013805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.014484</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.014565</td>\n",
       "      <td>0.017958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.015910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.016840</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>0.020730</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.017355</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.010213</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>0.017546</td>\n",
       "      <td>0.022361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10104.0   10107.0   11308.0   11850.0   12060.0   12490.0   13856.0  \\\n",
       "0  0.010526  0.011005  0.010056  0.012633  0.018416  0.010706  0.012357   \n",
       "1  0.011350  0.010025  0.009605  0.013825  0.017108  0.010282  0.011580   \n",
       "2  0.011471  0.009882  0.009589  0.013173  0.014819  0.029035  0.010699   \n",
       "3  0.011883  0.010398  0.008824  0.013408  0.014484  0.022256  0.010214   \n",
       "4  0.015279  0.011648  0.012030  0.016840  0.016273  0.020730  0.014009   \n",
       "\n",
       "    13901.0   13928.0   14008.0  ...   65875.0   66093.0   66181.0   66800.0  \\\n",
       "0  0.011636  0.012430  0.021510  ...  0.011018  0.009820  0.011313  0.012489   \n",
       "1  0.012084  0.015943  0.019810  ...  0.010203  0.009727  0.010932  0.012889   \n",
       "2  0.011802  0.013677  0.021472  ...  0.010737  0.010371  0.010090  0.012209   \n",
       "3  0.010672  0.014565  0.017958  ...  0.010671  0.009398  0.010965  0.011592   \n",
       "4  0.014059  0.017355  0.019015  ...  0.013227  0.010322  0.010213  0.011118   \n",
       "\n",
       "    70519.0   76076.0   77178.0   83443.0   84788.0   92655.0  \n",
       "0  0.017520  0.011489  0.013233  0.012022  0.018215  0.017688  \n",
       "1  0.015818  0.009538  0.013657  0.011978  0.016792  0.015412  \n",
       "2  0.015904  0.009866  0.012899  0.011923  0.018045  0.013805  \n",
       "3  0.017095  0.010086  0.012437  0.011046  0.017436  0.015910  \n",
       "4  0.017243  0.012850  0.014242  0.016875  0.017546  0.022361  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_volatility_table = pd.DataFrame(np.array(predicted_volatility_list).T)\n",
    "predicted_volatility_table.columns = list(bigcompany_list.values[:,0])\n",
    "predicted_volatility_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #table.to_csv(\"DSVM_NLL_test_200814.csv\",sep=',')\n",
    "# #predicted_volatility_table.to_csv(\"DSVM_outsample_vol_200814.csv\",sep=',')\n",
    "\n",
    "# table.to_csv(\"DSVM_NLL_test_200830.csv\",sep=',')\n",
    "# predicted_volatility_table.to_csv(\"DSVM_outsample_vol_200830.csv\",sep=',')\n",
    "\n",
    "table.to_csv(\"DSVM_NLL_test_200906_10.csv\",sep=',')\n",
    "predicted_volatility_table.to_csv(\"DSVM_outsample_vol_200906_10.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot out of sample predicted volatility for all companies (already calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSVM_outsample = pd.read_csv(\"DSVM_outsample_vol_200906_10.csv\").iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparemodel = \"EGARCH\"\n",
    "GARCH_outsample = pd.read_csv(comparemodel+\"_outsampel_vol.csv\").iloc[:,1:]\n",
    "\n",
    "for index_company in range(39):# 6#np.random.randint(39)\n",
    "    \n",
    "    PERMNO = bigcompany_list.values[index_company][0]\n",
    "    RawData = sp500_df.loc[sp500_df['PERMNO'] == PERMNO]\n",
    "    RawData.reset_index(inplace=True)\n",
    "    #RawData.head()\n",
    "    \n",
    "    GARCH_outsample_vol = GARCH_outsample.iloc[:,index_company].values\n",
    "    #GARCH_outsample_vol.shape\n",
    "\n",
    "    DSVM_outsample_vol = DSVM_outsample.iloc[:,index_company].values\n",
    "    #DSVM_outsample_vol.shape\n",
    "\n",
    "    length = 806#250\n",
    "    length_r = 806#outsample.shape[1]#np.random.randint(length,outsample.shape[1])\n",
    "    length_l = length_r - length\n",
    "\n",
    "#     Date = RawData.iloc[-outsample.shape[1]:]['Date'].values[length_l:length_r]\n",
    "#     price = RawData.iloc[-outsample.shape[1]:]['PRC'].values[length_l:length_r]\n",
    "#     returnx = RawData.iloc[-outsample.shape[1]:]['RETX'].values[length_l:length_r]\n",
    "    Date = RawData.iloc[-length:]['Date'].values[length_l:length_r]\n",
    "    price = RawData.iloc[-length:]['PRC'].values[length_l:length_r]\n",
    "    returnx = RawData.iloc[-length:]['RETX'].values[length_l:length_r]\n",
    "\n",
    "    y_std = DSVM_outsample_vol[length_l:length_r]\n",
    "    \n",
    "    label = 'Volatility (' + comparemodel + ')'\n",
    "\n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(20, 6),sharex = True);\n",
    "\n",
    "    ax1.plot(price,alpha=1,label = \"price\");\n",
    "\n",
    "    ax2.plot(returnx,alpha=1,label='true return');\n",
    "\n",
    "    ax3.plot(y_std,alpha=1,color='#ff7f0e',label='Volatility (DSVM)');\n",
    "    ax3.plot(GARCH_outsample_vol[length_l:length_r],alpha=1,color='green',label = label);\n",
    "\n",
    "    ax1.legend();\n",
    "    ax2.legend();\n",
    "    ax3.legend();\n",
    "\n",
    "    # ax3.set_xticks(np.arange(0,outsample.shape[1],100))\n",
    "    # ax3.set_xticklabels(Date[np.arange(0,outsample.shape[1],100)])\n",
    "\n",
    "    ax2.set_xticks(np.arange(0,length,int(length/5)));\n",
    "    ax2.set_xticklabels(Date[np.arange(0,length,int(length/5))]);\n",
    "\n",
    "    #plt.suptitle(\"Out of sample\");\"Recursive one-day-ahead forecasting\"\n",
    "    \n",
    "    print(index_company,PERMNO)\n",
    "    plt.savefig(\"20200908_images_\"+ comparemodel + \"/stock%i_outsample\"%index_company,bbox_inches='tight');\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
